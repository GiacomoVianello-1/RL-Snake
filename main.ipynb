{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473c0227",
   "metadata": {},
   "source": [
    "# Snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "import environments_fully_observable \n",
    "import environments_partially_observable\n",
    "from environments_fully_observable import OriginalSnakeEnvironment\n",
    "\n",
    "# Training utilities\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check for GPU availability (CUDA for Nvidia, MPS for Mac M1/M2/M3/M4)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ GPU Found: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"‚úÖ MPS (Metal Performance Shaders) Found: Apple Silicon Acceleration enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"‚ùå No GPU Found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894046e",
   "metadata": {},
   "source": [
    "## Environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# function to standardize getting an env for the whole notebook\n",
    "def get_env(n=1000):\n",
    "    # n is the number of boards that you want to simulate parallely\n",
    "    # size is the size of each board, also considering the borders\n",
    "    # mask for the partially observable, is the size of the local neighborhood\n",
    "    size = 7\n",
    "    e = environments_fully_observable.OriginalSnakeEnvironment(n, size)\n",
    "    # or environments_partially_observable.OriginalSnakeEnvironment(n, size, 2)\n",
    "    return e\n",
    "\n",
    "env_ = get_env()\n",
    "\n",
    "fig, axs = plt.subplots(1, min(len(env_.boards), 5), figsize=(10,3))\n",
    "for ax, board in zip(axs, env_.boards):\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.imshow(board, origin=\"lower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246aab9d",
   "metadata": {},
   "source": [
    "## Model DDQN\n",
    "\n",
    "### Definition of the model and utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d1841",
   "metadata": {},
   "source": [
    "For DDQN we define a Replay Buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daaa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.ptr = 0\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        # Memorize transitions (handles batch of inputs)\n",
    "        for i in range(len(state)):\n",
    "            if len(self.buffer) < self.capacity:\n",
    "                self.buffer.append(None)\n",
    "            self.buffer[self.ptr] = (state[i], action[i], reward[i], next_state[i], done[i])\n",
    "            self.ptr = (self.ptr + 1) % self.capacity # Circular buffer if full\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, ns, d = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(s, dtype=torch.float32, device=device),\n",
    "            torch.tensor(a, dtype=torch.int64, device=device), # Int64 for actions (indices)\n",
    "            torch.tensor(r, dtype=torch.float32, device=device).unsqueeze(1),\n",
    "            torch.tensor(ns, dtype=torch.float32, device=device),\n",
    "            torch.tensor(d, dtype=torch.float32, device=device).unsqueeze(1),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd3c9c",
   "metadata": {},
   "source": [
    "We create a safety mask to force the agent avoiding actions that lead to walls. This speed up training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947db60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safety_mask(env):\n",
    "    # Give a mask (N, 4) where actions that would lead to collisions are -1e9\n",
    "    masks = np.zeros((env.n_boards, 4), dtype=np.float32)\n",
    "    # Find heads: (N, 3) -> [board_idx, x, y]\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    \n",
    "    # Directions (dx, dy) based on env.move: UP(0):+1x, RIGHT(1):+1y, DOWN(2):-1x, LEFT(3):-1y\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "    \n",
    "    for i, (b, hx, hy) in enumerate(heads):\n",
    "        for action, (dx, dy) in moves.items():\n",
    "            nx, ny = hx + dx, hy + dy\n",
    "            # Check for collisions with WALLS (0) or BODY (3)\n",
    "            if env.boards[b, nx, ny] == env.WALL or env.boards[b, nx, ny] == env.BODY:\n",
    "                masks[b, action] = -1e9\n",
    "    return torch.tensor(masks, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99219a6",
   "metadata": {},
   "source": [
    "Create the CNN for the Q network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # Original input shape: (7, 7, 4) -> Channels: [Empty, Fruit, Body, Head]\n",
    "        # PyTorch wants (N, C, H, W), so (N, 4, 7, 7)\n",
    "        \n",
    "        # Conv2D(32, (3, 3), padding=\"same\")\n",
    "        # In PyTorch, padding=1 on kernel 3x3 keeps spatial dimension (7x7)\n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Conv2D(64, (3, 3), activation=\"relu\") -> no padding (valid)\n",
    "        # 7x7 -> kernel 3 -> 5x5\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        \n",
    "        # Flatten: 64 canali * 5 * 5 = 1600\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 128)\n",
    "        self.fc2 = nn.Linear(128, 4) # 4 Azioni\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert x in (N, 4, 7, 7) for PyTorch\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x) # Linear output (logits)\n",
    "\n",
    "# Initialize Q-Network and Target Network\n",
    "q = DQN().to(device)\n",
    "value = DQN().to(device) # Target Network\n",
    "value.load_state_dict(q.state_dict()) # copy weights\n",
    "agent = q # Alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7c2a9b",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Use Adam optimizer and reduce the number of active boards to focus the training and gather better data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8faa8b2",
   "metadata": {},
   "source": [
    "### Baseline + D3QN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_policy(env):\n",
    "    \"\"\"\n",
    "    Safe-Greedy Heuristic:\n",
    "    1. Move toward the fruit if the move is safe.\n",
    "    2. Otherwise choose the safe move that maximizes free space ahead.\n",
    "    3. If no safe moves exist, pick a random move.\n",
    "    \"\"\"\n",
    "    n = env.n_boards\n",
    "    actions = np.zeros((n, 1), dtype=np.int32)\n",
    "\n",
    "    # Get heads and fruits\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    fruits = np.argwhere(env.boards == env.FRUIT)\n",
    "\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "    fruits = fruits[fruits[:, 0].argsort()]\n",
    "\n",
    "    # Directions\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "\n",
    "    for i in range(n):\n",
    "        b, hx, hy = heads[i]\n",
    "        _, fx, fy = fruits[i]\n",
    "\n",
    "        # Preferred moves (toward fruit)\n",
    "        preferred = []\n",
    "        if fx > hx: preferred.append(0)  # UP\n",
    "        if fx < hx: preferred.append(2)  # DOWN\n",
    "        if fy > hy: preferred.append(1)  # RIGHT\n",
    "        if fy < hy: preferred.append(3)  # LEFT\n",
    "\n",
    "        # Safety mask\n",
    "        mask = get_safety_mask(env)[i].cpu().numpy()\n",
    "\n",
    "        # 1. Try preferred safe moves\n",
    "        for m in preferred:\n",
    "            if mask[m] == 0:\n",
    "                actions[i] = m\n",
    "                break\n",
    "        else:\n",
    "            # 2. Choose safe move with most free space\n",
    "            safe_moves = [m for m in range(4) if mask[m] == 0]\n",
    "\n",
    "            if safe_moves:\n",
    "                best_move = None\n",
    "                best_space = -1\n",
    "\n",
    "                for m in safe_moves:\n",
    "                    dx, dy = moves[m]\n",
    "                    nx, ny = hx + dx, hy + dy\n",
    "\n",
    "                    # Count free space in that direction\n",
    "                    space = 0\n",
    "                    while env.boards[b, nx, ny] == env.EMPTY:\n",
    "                        space += 1\n",
    "                        nx += dx\n",
    "                        ny += dy\n",
    "\n",
    "                    if space > best_space:\n",
    "                        best_space = space\n",
    "                        best_move = m\n",
    "\n",
    "                actions[i] = best_move\n",
    "\n",
    "            else:\n",
    "                # 3. No safe moves ‚Üí random fallback\n",
    "                actions[i] = np.random.randint(0, 4)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def evaluate_baseline(steps=500):\n",
    "    env = get_env(n=500)\n",
    "    fruits = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        actions = baseline_policy(env)\n",
    "        rewards = env.move(actions)\n",
    "        rewards = rewards.numpy().flatten() if hasattr(rewards, \"numpy\") else rewards\n",
    "        fruits.append(np.sum(rewards > 0.5))\n",
    "\n",
    "    return np.array(fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06528774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0. Utility: reset singola board\n",
    "# ---------------------------------------------------\n",
    "def reset_single_board(env, b):\n",
    "    size = env.board_size\n",
    "\n",
    "    board = np.ones((size, size)) * env.EMPTY\n",
    "    board[[0, -1], :] = env.WALL\n",
    "    board[:, [0, -1]] = env.WALL\n",
    "\n",
    "    # HEAD\n",
    "    available = np.argwhere(board == env.EMPTY)\n",
    "    ind = available[np.random.choice(len(available))]\n",
    "    board[ind[0], ind[1]] = env.HEAD\n",
    "\n",
    "    # FRUIT\n",
    "    available = np.argwhere(board == env.EMPTY)\n",
    "    ind = available[np.random.choice(len(available))]\n",
    "    board[ind[0], ind[1]] = env.FRUIT\n",
    "\n",
    "    env.boards[b] = board\n",
    "    env.bodies[b] = []\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Safety mask (come prima)\n",
    "# ---------------------------------------------------\n",
    "def get_safety_mask(env):\n",
    "    masks = np.zeros((env.n_boards, 4), dtype=np.float32)\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "\n",
    "    for _, (b, hx, hy) in enumerate(heads):\n",
    "        for action, (dx, dy) in moves.items():\n",
    "            nx, ny = hx + dx, hy + dy\n",
    "            # opzionale: controllo bounds\n",
    "            if not (0 <= nx < env.board_size and 0 <= ny < env.board_size):\n",
    "                masks[b, action] = -1e9\n",
    "                continue\n",
    "            if env.boards[b, nx, ny] == env.WALL or env.boards[b, nx, ny] == env.BODY:\n",
    "                masks[b, action] = -1e9\n",
    "    return torch.tensor(masks, device=device)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Architettura Dueling DQN potenziata\n",
    "# ---------------------------------------------------\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, board_size=7): \n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.fc_input_dim = 128 * 5 * 5 \n",
    "        \n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1) \n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 4) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        val = self.value_stream(x)\n",
    "        adv = self.advantage_stream(x)\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Baseline euristica safe-greedy\n",
    "# ---------------------------------------------------\n",
    "def baseline_policy(env):\n",
    "    \"\"\"\n",
    "    Safe-Greedy Heuristic:\n",
    "    1. Move toward the fruit if the move is safe.\n",
    "    2. Otherwise choose the safe move that maximizes free space ahead.\n",
    "    3. If no safe moves exist, pick a random move.\n",
    "    \"\"\"\n",
    "    n = env.n_boards\n",
    "    actions = np.zeros((n, 1), dtype=np.int32)\n",
    "\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    fruits = np.argwhere(env.boards == env.FRUIT)\n",
    "\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "    fruits = fruits[fruits[:, 0].argsort()]\n",
    "\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "    mask_all = get_safety_mask(env).cpu().numpy()\n",
    "\n",
    "    for i in range(n):\n",
    "        b, hx, hy = heads[i]\n",
    "        _, fx, fy = fruits[i]\n",
    "\n",
    "        preferred = []\n",
    "        if fx > hx: preferred.append(0)  # UP\n",
    "        if fx < hx: preferred.append(2)  # DOWN\n",
    "        if fy > hy: preferred.append(1)  # RIGHT\n",
    "        if fy < hy: preferred.append(3)  # LEFT\n",
    "\n",
    "        mask = mask_all[i]\n",
    "\n",
    "        # 1. Preferred safe move\n",
    "        chosen = None\n",
    "        for m in preferred:\n",
    "            if mask[m] == 0:\n",
    "                chosen = m\n",
    "                break\n",
    "\n",
    "        if chosen is None:\n",
    "            # 2. Safe move with max free space\n",
    "            safe_moves = [m for m in range(4) if mask[m] == 0]\n",
    "            if safe_moves:\n",
    "                best_move = None\n",
    "                best_space = -1\n",
    "                for m in safe_moves:\n",
    "                    dx, dy = moves[m]\n",
    "                    nx, ny = hx + dx, hy + dy\n",
    "                    space = 0\n",
    "                    while (0 <= nx < env.board_size and 0 <= ny < env.board_size and\n",
    "                           env.boards[b, nx, ny] == env.EMPTY):\n",
    "                        space += 1\n",
    "                        nx += dx\n",
    "                        ny += dy\n",
    "                    if space > best_space:\n",
    "                        best_space = space\n",
    "                        best_move = m\n",
    "                chosen = best_move\n",
    "            else:\n",
    "                # 3. No safe moves ‚Üí random fallback\n",
    "                chosen = np.random.randint(0, 4)\n",
    "\n",
    "        actions[i] = chosen\n",
    "\n",
    "    return actions\n",
    "\n",
    "def evaluate_baseline(steps=1000):\n",
    "    env = get_env(n=500)\n",
    "    fruits = []\n",
    "    for _ in range(steps):\n",
    "        actions = baseline_policy(env)\n",
    "        rewards = env.move(actions)\n",
    "        rewards = rewards.numpy().flatten() if hasattr(rewards, \"numpy\") else rewards\n",
    "        fruits.append(np.sum(rewards > 0.5))\n",
    "    return np.array(fruits)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Setup RL\n",
    "# ---------------------------------------------------\n",
    "NUM_BOARDS = 500 \n",
    "env_ = get_env(n=NUM_BOARDS)\n",
    "\n",
    "q = DuelingDQN().to(device)       \n",
    "value = DuelingDQN().to(device)   \n",
    "value.load_state_dict(q.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer(capacity=200000) \n",
    "\n",
    "TOTAL_ITERATIONS = 10000\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "MIN_EPSILON = 0.05\n",
    "epsilon_decrement = (EPSILON - MIN_EPSILON) / (TOTAL_ITERATIONS * 0.7)\n",
    "\n",
    "rl_rewards_history = []\n",
    "moving_avg_history = []\n",
    "window_size = 100 \n",
    "\n",
    "fruits_history = []\n",
    "wall_deaths_history = []\n",
    "self_eat_history = []\n",
    "avg_length_history = []\n",
    "alive_ratio_history = []\n",
    "\n",
    "# --- variabili per reward shaping ---\n",
    "last_actions = np.full(NUM_BOARDS, -1, dtype=np.int32)\n",
    "no_fruit_counter = np.zeros(NUM_BOARDS, dtype=np.int32)\n",
    "fruit_streak = np.zeros(NUM_BOARDS, dtype=np.int32)\n",
    "\n",
    "print(\"üöÄ AVVIO TRAINING 'PURE ENV DESIGN'...\")\n",
    "print(\"Valori Env: Muro -1.0 | Frutto +1.0 | Step -0.01 | Self -0.5\")\n",
    "\n",
    "q.train()\n",
    "state = env_.to_state()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Training loop\n",
    "# ---------------------------------------------------\n",
    "for iteration in trange(TOTAL_ITERATIONS, desc=\"Training\"):\n",
    "    \n",
    "    # A. Mossa\n",
    "    state_tensor = torch.tensor(state, device=device).float()\n",
    "    mask = get_safety_mask(env_)\n",
    "    \n",
    "    if random.random() < EPSILON:\n",
    "        actions_np = np.random.randint(0, 4, size=(NUM_BOARDS, 1)).astype(np.int32)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q(state_tensor)\n",
    "            if iteration < 2000:\n",
    "                actions = torch.argmax(q_values + mask, dim=1)\n",
    "            else:\n",
    "                actions = torch.argmax(q_values, dim=1)\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    # 1. Esegui la mossa\n",
    "    rewards_tensor = env_.move(actions_np)\n",
    "    if hasattr(rewards_tensor, 'numpy'):\n",
    "        rewards_np = rewards_tensor.numpy().flatten()\n",
    "    else:\n",
    "        rewards_np = np.array(rewards_tensor).flatten()\n",
    "        \n",
    "    # 2. Stato conseguente (anche se \"morto\")\n",
    "    next_state_dead = env_.to_state()\n",
    "    \n",
    "    # 3. Reward puri\n",
    "    raw_rewards = rewards_np.astype(np.float32)\n",
    "\n",
    "    # --- REWARD SHAPING ---\n",
    "    # 1) Survival bonus\n",
    "    raw_rewards += 0.002\n",
    "\n",
    "    # 2) Fruit eaten mask\n",
    "    fruit_mask = (raw_rewards > 0.5)\n",
    "\n",
    "    # 3) Fruit streak bonus\n",
    "    fruit_streak[fruit_mask] += 1\n",
    "    fruit_streak[~fruit_mask] = 0\n",
    "    raw_rewards += 0.1 * fruit_streak\n",
    "\n",
    "    # 4) Stagnation penalty\n",
    "    no_fruit_counter[fruit_mask] = 0\n",
    "    no_fruit_counter[~fruit_mask] += 1\n",
    "    raw_rewards[no_fruit_counter > 20] -= 0.05\n",
    "\n",
    "    # 5) Oscillation penalty\n",
    "    current_actions = actions_np.flatten()\n",
    "    opposite = {0:2, 2:0, 1:3, 3:1}\n",
    "    osc_mask = np.array([\n",
    "        (last_actions[i] == opposite.get(current_actions[i], -1))\n",
    "        for i in range(NUM_BOARDS)\n",
    "    ])\n",
    "    raw_rewards[osc_mask] -= 0.03\n",
    "    last_actions = current_actions.copy()\n",
    "\n",
    "    # --- METRICHE ---\n",
    "    fruits_eaten = np.sum(raw_rewards > 0.5)\n",
    "    wall_deaths = np.sum(raw_rewards < -0.9)\n",
    "    self_eat = np.sum((raw_rewards < -0.1) & (raw_rewards > -0.9))\n",
    "    avg_length = np.mean([len(body) for body in env_.bodies])\n",
    "    alive_ratio = 1.0 - (wall_deaths / NUM_BOARDS)\n",
    "\n",
    "    fruits_history.append(fruits_eaten)\n",
    "    wall_deaths_history.append(wall_deaths)\n",
    "    self_eat_history.append(self_eat)\n",
    "    avg_length_history.append(avg_length)\n",
    "    alive_ratio_history.append(alive_ratio)\n",
    "\n",
    "    # 4. Done (muro = morte)\n",
    "    hit_wall = (rewards_np < -0.9)\n",
    "    dones_np = hit_wall.astype(np.float32)\n",
    "\n",
    "    # 5. Respawn manuale delle board morte\n",
    "    dead_boards = np.where(hit_wall)[0]\n",
    "    for b in dead_boards:\n",
    "        reset_single_board(env_, b)\n",
    "\n",
    "    # 6. Salviamo nel buffer la transizione \"vera\"\n",
    "    replay_buffer.store(state, actions_np, raw_rewards, next_state_dead, dones_np)\n",
    "\n",
    "    # 7. Stato per il prossimo step\n",
    "    state = env_.to_state()\n",
    "    \n",
    "    # 8. Epsilon decay\n",
    "    if EPSILON > MIN_EPSILON:\n",
    "        EPSILON -= epsilon_decrement\n",
    "\n",
    "    rl_rewards_history.append(np.mean(raw_rewards))\n",
    "\n",
    "    # D. Learning\n",
    "    if len(replay_buffer) > 5000:\n",
    "        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        current_q = q(b_s).gather(1, b_a)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_online = q(b_ns)\n",
    "            best_actions = torch.argmax(next_q_online, dim=1, keepdim=True)\n",
    "            next_q_target = value(b_ns)\n",
    "            double_q_value = next_q_target.gather(1, best_actions)\n",
    "            target_v = b_r + (GAMMA * double_q_value * (1.0 - b_d))\n",
    "        \n",
    "        loss = F.smooth_l1_loss(current_q, target_v)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(q.parameters(), 10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        value.load_state_dict(q.state_dict())\n",
    "\n",
    "    # E. Monitor\n",
    "    if iteration % 200 == 0 and iteration > 0:\n",
    "        current_avg = np.mean(rl_rewards_history[-window_size:])\n",
    "        moving_avg_history.append(current_avg)\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        \n",
    "        # Reward media\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(rl_rewards_history, alpha=0.15, color='gray')\n",
    "        plt.plot(np.linspace(0, len(rl_rewards_history), len(moving_avg_history)),\n",
    "                 moving_avg_history, color='orange', linewidth=2)\n",
    "        plt.title(f\"Reward media (iter {iteration})\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Frutti mangiati\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(fruits_history, color='green')\n",
    "        plt.title(\"Frutti mangiati per iterazione\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Morti contro il muro\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(wall_deaths_history, color='red')\n",
    "        plt.title(\"Morti contro il muro\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Lunghezza media del serpente\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(avg_length_history, color='blue')\n",
    "        plt.title(\"Lunghezza media del serpente\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "agent = q \n",
    "print(\"Training 'Pure Environment' completato.\")\n",
    "print(\"Frutti ultimi 500:\", np.mean(fruits_history[-500:]))\n",
    "print(\"Morti muro ultimi 500:\", np.mean(wall_deaths_history[-500:]))\n",
    "print(\"Lunghezza media ultimi 500:\", np.mean(avg_length_history[-500:]))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Confronto finale RL vs baseline\n",
    "# ---------------------------------------------------\n",
    "baseline_fruits = evaluate_baseline(steps=1000)\n",
    "baseline_value = np.mean(baseline_fruits)\n",
    "\n",
    "window = 200\n",
    "rl_moving = np.convolve(fruits_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(rl_moving, label=\"RL Agent (moving avg)\", color=\"blue\")\n",
    "plt.axhline(y=baseline_value, color=\"orange\", linestyle=\"--\", label=\"Baseline (constant)\")\n",
    "\n",
    "plt.title(\"Learning Curve: Fruits per Iteration (RL vs Baseline)\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Fruits per Iteration\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline average fruits per step: {baseline_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfe987",
   "metadata": {},
   "source": [
    "### Ultimate version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 0. Reset singola board\n",
    "# ---------------------------------------------------\n",
    "def reset_single_board(env, b):\n",
    "    size = env.board_size\n",
    "\n",
    "    board = np.ones((size, size)) * env.EMPTY\n",
    "    board[[0, -1], :] = env.WALL\n",
    "    board[:, [0, -1]] = env.WALL\n",
    "\n",
    "    # HEAD\n",
    "    available = np.argwhere(board == env.EMPTY)\n",
    "    ind = available[np.random.choice(len(available))]\n",
    "    board[ind[0], ind[1]] = env.HEAD\n",
    "\n",
    "    # FRUIT\n",
    "    available = np.argwhere(board == env.EMPTY)\n",
    "    ind = available[np.random.choice(len(available))]\n",
    "    board[ind[0], ind[1]] = env.FRUIT\n",
    "\n",
    "    env.boards[b] = board\n",
    "    env.bodies[b] = []\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Safety mask\n",
    "# ---------------------------------------------------\n",
    "def get_safety_mask(env):\n",
    "    masks = np.zeros((env.n_boards, 4), dtype=np.float32)\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "\n",
    "    for _, (b, hx, hy) in enumerate(heads):\n",
    "        for action, (dx, dy) in moves.items():\n",
    "            nx, ny = hx + dx, hy + dy\n",
    "            if not (0 <= nx < env.board_size and 0 <= ny < env.board_size):\n",
    "                masks[b, action] = -1e9\n",
    "                continue\n",
    "            if env.boards[b, nx, ny] == env.WALL or env.boards[b, nx, ny] == env.BODY:\n",
    "                masks[b, action] = -1e9\n",
    "    return torch.tensor(masks, device=device)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Dueling DQN (potenziato ma semplice)\n",
    "# ---------------------------------------------------\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, board_size=7): \n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3)\n",
    "        self.fc_input_dim = 128 * 5 * 5 \n",
    "        \n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1) \n",
    "        )\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.fc_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        val = self.value_stream(x)\n",
    "        adv = self.advantage_stream(x)\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Baseline euristica safe-greedy\n",
    "# ---------------------------------------------------\n",
    "def baseline_policy(env):\n",
    "    n = env.n_boards\n",
    "    actions = np.zeros((n, 1), dtype=np.int32)\n",
    "\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    fruits = np.argwhere(env.boards == env.FRUIT)\n",
    "\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "    fruits = fruits[fruits[:, 0].argsort()]\n",
    "\n",
    "    moves = {0: (1, 0), 1: (0, 1), 2: (-1, 0), 3: (0, -1)}\n",
    "    mask_all = get_safety_mask(env).cpu().numpy()\n",
    "\n",
    "    for i in range(n):\n",
    "        b, hx, hy = heads[i]\n",
    "        _, fx, fy = fruits[i]\n",
    "\n",
    "        preferred = []\n",
    "        if fx > hx: preferred.append(0)\n",
    "        if fx < hx: preferred.append(2)\n",
    "        if fy > hy: preferred.append(1)\n",
    "        if fy < hy: preferred.append(3)\n",
    "\n",
    "        mask = mask_all[i]\n",
    "\n",
    "        # 1. Preferred safe move\n",
    "        for m in preferred:\n",
    "            if mask[m] == 0:\n",
    "                actions[i] = m\n",
    "                break\n",
    "        else:\n",
    "            # 2. Any safe move\n",
    "            safe_moves = [m for m in range(4) if mask[m] == 0]\n",
    "            if safe_moves:\n",
    "                actions[i] = random.choice(safe_moves)\n",
    "            else:\n",
    "                actions[i] = np.random.randint(0, 4)\n",
    "\n",
    "    return actions\n",
    "\n",
    "def evaluate_baseline(steps=1000):\n",
    "    env = get_env(n=500)\n",
    "    fruits = []\n",
    "    for _ in range(steps):\n",
    "        actions = baseline_policy(env)\n",
    "        rewards = env.move(actions)\n",
    "        rewards = rewards.numpy().flatten() if hasattr(rewards, \"numpy\") else rewards\n",
    "        fruits.append(np.sum(rewards > 0.5))\n",
    "    return np.array(fruits)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Setup RL\n",
    "# ---------------------------------------------------\n",
    "NUM_BOARDS = 500 \n",
    "env_ = get_env(n=NUM_BOARDS)\n",
    "\n",
    "q = DuelingDQN().to(device)       \n",
    "value = DuelingDQN().to(device)   \n",
    "value.load_state_dict(q.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(q.parameters(), lr=1e-4)\n",
    "replay_buffer = ReplayBuffer(capacity=200000) \n",
    "\n",
    "TOTAL_ITERATIONS = 10000\n",
    "BATCH_SIZE = 512\n",
    "GAMMA = 0.95\n",
    "EPSILON = 1.0\n",
    "MIN_EPSILON = 0.05\n",
    "epsilon_decrement = (EPSILON - MIN_EPSILON) / (TOTAL_ITERATIONS * 0.7)\n",
    "\n",
    "rl_rewards_history = []\n",
    "fruits_history = []\n",
    "wall_deaths_history = []\n",
    "avg_length_history = []\n",
    "\n",
    "rl_rewards_history = []\n",
    "moving_avg_history = []\n",
    "window_size = 100\n",
    "\n",
    "fruits_history = []\n",
    "wall_deaths_history = []\n",
    "avg_length_history = []\n",
    "\n",
    "\n",
    "# Reward shaping: solo anti-stagnazione\n",
    "no_fruit_counter = np.zeros(NUM_BOARDS, dtype=np.int32)\n",
    "\n",
    "print(\"üöÄ Training RL Agent...\")\n",
    "\n",
    "q.train()\n",
    "state = env_.to_state()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Training loop\n",
    "# ---------------------------------------------------\n",
    "for iteration in trange(TOTAL_ITERATIONS, desc=\"Training\"):\n",
    "    \n",
    "    state_tensor = torch.tensor(state, device=device).float()\n",
    "    mask = get_safety_mask(env_)\n",
    "\n",
    "    # Epsilon-greedy\n",
    "    if random.random() < EPSILON:\n",
    "        actions_np = np.random.randint(0, 4, size=(NUM_BOARDS, 1)).astype(np.int32)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = q(state_tensor)\n",
    "            actions = torch.argmax(q_values + mask, dim=1)\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "    # Step\n",
    "    rewards_tensor = env_.move(actions_np)\n",
    "    rewards_np = rewards_tensor.numpy().flatten() if hasattr(rewards_tensor, \"numpy\") else np.array(rewards_tensor).flatten()\n",
    "    next_state_dead = env_.to_state()\n",
    "\n",
    "    raw_rewards = rewards_np.astype(np.float32)\n",
    "\n",
    "    # --- Reward shaping minimale ---\n",
    "    raw_rewards += 0.002  # survival bonus\n",
    "\n",
    "    fruit_mask = (raw_rewards > 0.5)\n",
    "    no_fruit_counter[fruit_mask] = 0\n",
    "    no_fruit_counter[~fruit_mask] += 1\n",
    "    raw_rewards[no_fruit_counter > 20] -= 0.05\n",
    "\n",
    "    # Metriche\n",
    "    fruits_history.append(np.sum(raw_rewards > 0.5))\n",
    "    wall_deaths_history.append(np.sum(raw_rewards < -0.9))\n",
    "    avg_length_history.append(np.mean([len(body) for body in env_.bodies]))\n",
    "\n",
    "    # Done\n",
    "    hit_wall = (rewards_np < -0.9)\n",
    "    dones_np = hit_wall.astype(np.float32)\n",
    "\n",
    "    # Respawn\n",
    "    for b in np.where(hit_wall)[0]:\n",
    "        reset_single_board(env_, b)\n",
    "\n",
    "    # Store transition\n",
    "    replay_buffer.store(state, actions_np, raw_rewards, next_state_dead, dones_np)\n",
    "\n",
    "    # Next state\n",
    "    state = env_.to_state()\n",
    "\n",
    "    # Epsilon decay\n",
    "    if EPSILON > MIN_EPSILON:\n",
    "        EPSILON -= epsilon_decrement\n",
    "\n",
    "    # Learning\n",
    "    if len(replay_buffer) > 5000:\n",
    "        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        current_q = q(b_s).gather(1, b_a)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_online = q(b_ns)\n",
    "            best_actions = torch.argmax(next_q_online, dim=1, keepdim=True)\n",
    "            next_q_target = value(b_ns)\n",
    "            double_q_value = next_q_target.gather(1, best_actions)\n",
    "            target_v = b_r + (GAMMA * double_q_value * (1.0 - b_d))\n",
    "        \n",
    "        loss = F.smooth_l1_loss(current_q, target_v)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(q.parameters(), 10.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    if iteration % 1000 == 0:\n",
    "        value.load_state_dict(q.state_dict())\n",
    "\n",
    "        # ---------------------------------------------------\n",
    "    # LIVE PLOTTING OGNI 200 ITERAZIONI\n",
    "    # ---------------------------------------------------\n",
    "    if iteration % 200 == 0 and iteration > 0:\n",
    "\n",
    "        # Moving average del reward\n",
    "        current_avg = np.mean(rl_rewards_history[-window_size:])\n",
    "        moving_avg_history.append(current_avg)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(14, 10))\n",
    "\n",
    "        # 1. Reward medio\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(rl_rewards_history, alpha=0.15, color='gray')\n",
    "        plt.plot(\n",
    "            np.linspace(0, len(rl_rewards_history), len(moving_avg_history)),\n",
    "            moving_avg_history,\n",
    "            color='orange',\n",
    "            linewidth=2\n",
    "        )\n",
    "        plt.title(f\"Reward medio (iter {iteration})\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 2. Frutti mangiati\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(fruits_history, color='green')\n",
    "        plt.title(\"Frutti mangiati per iterazione\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Morti contro il muro\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(wall_deaths_history, color='red')\n",
    "        plt.title(\"Morti contro il muro\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # 4. Lunghezza media del serpente\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(avg_length_history, color='blue')\n",
    "        plt.title(\"Lunghezza media del serpente\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "agent = q\n",
    "print(\"Training completato.\")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Confronto RL vs baseline\n",
    "# ---------------------------------------------------\n",
    "baseline_fruits = evaluate_baseline(steps=1000)\n",
    "baseline_value = np.mean(baseline_fruits)\n",
    "\n",
    "window = 200\n",
    "rl_moving = np.convolve(fruits_history, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(rl_moving, label=\"RL Agent (moving avg)\", color=\"blue\")\n",
    "plt.axhline(y=baseline_value, color=\"orange\", linestyle=\"--\", label=\"Baseline\")\n",
    "\n",
    "plt.title(\"Learning Curve: Fruits per Iteration (RL vs Baseline)\")\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Fruits per Iteration\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Baseline average fruits per step: {baseline_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e8515",
   "metadata": {},
   "source": [
    "Now we have trained the model and we can save the weights and reuse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64646df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights of the online Q-network\n",
    "torch.save(q.state_dict(), 'snake_ddqn_weights_new.pth')\n",
    "print(\"Weights correctly saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3e7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reuse them later:\n",
    "\n",
    "# 1. Create the model with the same architecture\n",
    "q = DQN().to(device)\n",
    "\n",
    "# 2. Load the weights\n",
    "q.load_state_dict(torch.load('snake_ddqn_weights_new.pth', map_location=device))\n",
    "\n",
    "# 3. Synchronize the target network\n",
    "value = DQN().to(device)\n",
    "value.load_state_dict(q.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419929",
   "metadata": {},
   "source": [
    "## Validation And Metrics\n",
    "\n",
    "We use 3 metrics: \n",
    "1. Check if the agent has learnt how to behave (i.e., if it eats some fruits).\n",
    "2. Check the average reward it gets from the environment.\n",
    "3. Check if it actually accomplishes the task by running the policy for some steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788953b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(model, steps=200):\n",
    "    model.eval() # Imposta il modello in modalit√† valutazione (no dropout, ecc.)\n",
    "    \n",
    "    # Create a new clean environment\n",
    "    test_env = get_env(n=100) \n",
    "    state = test_env.to_state()\n",
    "    \n",
    "    total_rewards = 0\n",
    "    fruits_eaten = 0\n",
    "    \n",
    "    # Execute for some steps without exploration (greedy)\n",
    "    for _ in range(steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Since we use a safety mask during training, we need it here too\n",
    "        mask = get_safety_mask(test_env) \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Just pure prediction (epsilon=0)\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "            # Apply the mask\n",
    "            actions = torch.argmax(q_values + mask, dim=1)\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "        \n",
    "        rewards_tensor = test_env.move(actions_np)\n",
    "        state = test_env.to_state()\n",
    "        \n",
    "        # Statistics\n",
    "        if hasattr(rewards_tensor, 'numpy'):\n",
    "             r_np = rewards_tensor.numpy().flatten()\n",
    "        else:\n",
    "             r_np = np.array(rewards_tensor).flatten()\n",
    "             \n",
    "        total_rewards += np.sum(r_np)\n",
    "        # Count how many times the reward is positive --> snake ate fruit\n",
    "        fruits_eaten += np.sum(r_np > 0)\n",
    "\n",
    "    avg_score = total_rewards / 100.0 # Average per board\n",
    "    avg_fruits = fruits_eaten / 100.0\n",
    "    \n",
    "    print(\"--- TESTS RESULTS (on 100 parallel snakes) ---\")\n",
    "    print(f\"Average Total Reward: {avg_score:.2f}\")\n",
    "    print(f\"Average Fruits Eaten: {avg_fruits:.2f}\")\n",
    "    \n",
    "    if avg_fruits > 1.0:\n",
    "        print(\"‚úÖ SUCCESS: Agent learnt how to play and seeks food!\")\n",
    "    elif avg_score > 0:\n",
    "        print(\"‚ö†Ô∏è WARNING: Agent survives but eats little.\")\n",
    "    else:\n",
    "        print(\"‚ùå FAILURE: Agent dies often or moves in circles.\")\n",
    "\n",
    "test_agent(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f2650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rl_rewards_history' in locals() and len(rl_rewards_history) > 0:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Compute smoothed rewards for better visualization\n",
    "    window = 50 # Describes the smooting window for the average\n",
    "    data = np.array(rl_rewards_history)\n",
    "    if len(data) >= window:\n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        plt.plot(smoothed, label='Average Reward (Smoothed)')\n",
    "    else:\n",
    "        plt.plot(data, label='Average Reward')\n",
    "        \n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836c62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_game(model, max_steps=100):\n",
    "    # Create a single environment for visualization\n",
    "    game_env = get_env(n=1) \n",
    "    state = game_env.to_state()\n",
    "    frames = []\n",
    "    \n",
    "    # Save initial frame\n",
    "    frames.append(game_env.boards[0].copy())\n",
    "    \n",
    "    print(\"Generating animation...\", end=\"\")\n",
    "    \n",
    "    # Mettiamo il modello in eval mode (buona norma in PyTorch)\n",
    "    model.eval()\n",
    "    \n",
    "    # Game Loop\n",
    "    for _ in range(max_steps):\n",
    "        # Convertiamo in Tensor PyTorch e spostiamo sul device (GPU/MPS/CPU)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Use the mask for consistency (restituisce gi√† un tensore sul device corretto)\n",
    "        mask = get_safety_mask(game_env)\n",
    "        \n",
    "        # Greedy prediction\n",
    "        with torch.no_grad(): # Disabilita il calcolo dei gradienti per l'inferenza\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "            # Argmax su dimensione 1\n",
    "            actions = torch.argmax(q_values + mask, dim=1)\n",
    "            \n",
    "            # Convertiamo in numpy (importante: .cpu() se siamo su GPU/MPS)\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "        \n",
    "        # Step\n",
    "        game_env.move(actions_np)\n",
    "        state = game_env.to_state()\n",
    "        \n",
    "        # Save the frame (copy necessary)\n",
    "        frames.append(game_env.boards[0].copy())\n",
    "        \n",
    "        # If the game is over (only walls left or board reset), we could stop, but the environment\n",
    "        # resets automatically, so we continue.\n",
    "\n",
    "    # Setup animation\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Color map:\n",
    "    # 0=Wall, 1=Empty, 2=Fruit, 3=Body, 4=Head\n",
    "\n",
    "    img = ax.imshow(frames[0], origin='lower', cmap='viridis', vmin=0, vmax=4)\n",
    "    ax.set_title(\"Snake Agent Replay\")\n",
    "\n",
    "    # Animation update function\n",
    "    def update(frame):\n",
    "        img.set_array(frame)\n",
    "        return [img]\n",
    "\n",
    "    # Create video\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, \n",
    "        update, \n",
    "        frames=frames, \n",
    "        interval=100, # milliseconds between frames (lower = faster)\n",
    "        blit=True\n",
    "    )\n",
    "    \n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())\n",
    "\n",
    "# Execute the visualization for 500 steps\n",
    "display_game(agent, max_steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f05a61",
   "metadata": {},
   "source": [
    "### Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54af66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(agent_model, steps=500):\n",
    "    print(f\"Running Benchmark for {steps} steps...\")\n",
    "    \n",
    "    env_ai = get_env(n=100)\n",
    "    env_heur = get_env(n=100)\n",
    "    \n",
    "    state_ai = env_ai.to_state()\n",
    "    \n",
    "    # Metriche\n",
    "    ai_rewards, heur_rewards = [], []\n",
    "    ai_fruits, heur_fruits = [], []\n",
    "    ai_wall, heur_wall = [], []\n",
    "    ai_self, heur_self = [], []\n",
    "    ai_len, heur_len = [], []\n",
    "    \n",
    "    agent_model.eval()\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        # === AI MOVE ===\n",
    "        state_tensor = torch.tensor(state_ai, dtype=torch.float32, device=device)\n",
    "        mask_ai = get_safety_mask(env_ai)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = agent_model(state_tensor)\n",
    "            actions_ai = torch.argmax(q_values + mask_ai, dim=1)\n",
    "            actions_ai_np = actions_ai.cpu().numpy().reshape(-1, 1)\n",
    "        \n",
    "        rewards_ai = env_ai.move(actions_ai_np)\n",
    "        state_ai = env_ai.to_state()\n",
    "        \n",
    "        # === HEURISTIC MOVE ===\n",
    "        actions_heur_np = heuristic_policy(env_heur)\n",
    "        rewards_heur = env_heur.move(actions_heur_np)\n",
    "        \n",
    "        # Convert rewards\n",
    "        rewards_ai = rewards_ai.numpy() if hasattr(rewards_ai, \"numpy\") else rewards_ai\n",
    "        rewards_heur = rewards_heur.numpy() if hasattr(rewards_heur, \"numpy\") else rewards_heur\n",
    "        \n",
    "        # === METRICHE ===\n",
    "        ai_rewards.append(np.mean(rewards_ai))\n",
    "        heur_rewards.append(np.mean(rewards_heur))\n",
    "        \n",
    "        ai_fruits.append(np.sum(rewards_ai > 0.5))\n",
    "        heur_fruits.append(np.sum(rewards_heur > 0.5))\n",
    "        \n",
    "        ai_wall.append(np.sum(rewards_ai < -0.9))\n",
    "        heur_wall.append(np.sum(rewards_heur < -0.9))\n",
    "        \n",
    "        ai_self.append(np.sum((rewards_ai < -0.1) & (rewards_ai > -0.9)))\n",
    "        heur_self.append(np.sum((rewards_heur < -0.1) & (rewards_heur > -0.9)))\n",
    "        \n",
    "        ai_len.append(np.mean([len(b) for b in env_ai.bodies]))\n",
    "        heur_len.append(np.mean([len(b) for b in env_heur.bodies]))\n",
    "    \n",
    "    return {\n",
    "        \"ai\": {\n",
    "            \"reward\": ai_rewards,\n",
    "            \"fruits\": ai_fruits,\n",
    "            \"wall\": ai_wall,\n",
    "            \"self\": ai_self,\n",
    "            \"length\": ai_len\n",
    "        },\n",
    "        \"heur\": {\n",
    "            \"reward\": heur_rewards,\n",
    "            \"fruits\": heur_fruits,\n",
    "            \"wall\": heur_wall,\n",
    "            \"self\": heur_self,\n",
    "            \"length\": heur_len\n",
    "        }\n",
    "    }\n",
    "\n",
    "res = run_benchmark(agent, steps=500)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(res[\"ai\"][\"fruits\"], label=\"AI\", color=\"blue\")\n",
    "plt.plot(res[\"heur\"][\"fruits\"], label=\"Heuristic\", color=\"orange\")\n",
    "plt.title(\"Fruits per step\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(res[\"ai\"][\"wall\"], label=\"AI\", color=\"blue\")\n",
    "plt.plot(res[\"heur\"][\"wall\"], label=\"Heuristic\", color=\"orange\")\n",
    "plt.title(\"Wall deaths per step\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.plot(res[\"ai\"][\"length\"], label=\"AI\", color=\"blue\")\n",
    "plt.plot(res[\"heur\"][\"length\"], label=\"Heuristic\", color=\"orange\")\n",
    "plt.title(\"Average snake length\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(np.cumsum(res[\"ai\"][\"reward\"]), label=\"AI\", color=\"blue\")\n",
    "plt.plot(np.cumsum(res[\"heur\"][\"reward\"]), label=\"Heuristic\", color=\"orange\")\n",
    "plt.title(\"Cumulative reward\")\n",
    "plt.legend(); plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35a97",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aadabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BOARDS = 1000\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Environment wrapper\n",
    "def make_env(n_boards=NUM_BOARDS, board_size=7):\n",
    "    return OriginalSnakeEnvironment(n_boards=n_boards, board_size=board_size)\n",
    "\n",
    "# Safety mask fuction: checks for unsafe moves for all boards\n",
    "def get_safety_mask(env):\n",
    "    masks = np.zeros((env.n_boards, 4), dtype=np.float32)\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "\n",
    "    moves = {0:(1,0), 1:(0,1), 2:(-1,0), 3:(0,-1)}\n",
    "\n",
    "    for b, hx, hy in heads:\n",
    "        for a, (dx, dy) in moves.items():\n",
    "            nx, ny = hx+dx, hy+dy\n",
    "            if env.boards[b, nx, ny] in (env.WALL, env.BODY):\n",
    "                masks[b, a] = 1.0   # unsafe\n",
    "    return torch.tensor(masks, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb8df3",
   "metadata": {},
   "source": [
    "Define the NN for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89346df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, board_size=7, n_channels=4, n_actions=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        self.board_size = board_size\n",
    "        conv_out_dim = 128 * board_size * board_size\n",
    "\n",
    "        self.fc = nn.Linear(conv_out_dim, 256)\n",
    "        self.policy_head = nn.Linear(256, n_actions)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = x.permute(0, 3, 1, 2)          # (N, H, W, C) -> (N, C, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def act(self, x, mask=None):\n",
    "        logits, value = self.forward(x)\n",
    "        if mask is not None:\n",
    "            logits = logits + mask\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action)\n",
    "        return action, logprob, value\n",
    "\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        logits, values = self.forward(x)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        logprobs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return logprobs, entropy, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout buffer for PPO\n",
    "@dataclass\n",
    "class RolloutBuffer:\n",
    "    obs: list\n",
    "    actions: list\n",
    "    logprobs: list\n",
    "    rewards: list\n",
    "    dones: list\n",
    "    values: list\n",
    "\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.values = []\n",
    "\n",
    "    def add(self, obs, action, logprob, reward, done, value):\n",
    "        # obs, reward, done, value, logprob, action are numpy or scalars\n",
    "        self.obs.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.logprobs.append(logprob)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def to_tensors(self):\n",
    "        # conver all to torch tensors\n",
    "        obs = torch.tensor(np.array(self.obs), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.long, device=device)\n",
    "        logprobs = torch.tensor(np.array(self.logprobs), dtype=torch.float32, device=device)\n",
    "        rewards = torch.tensor(np.array(self.rewards), dtype=torch.float32, device=device)\n",
    "        dones = torch.tensor(np.array(self.dones), dtype=torch.float32, device=device)\n",
    "        values = torch.tensor(np.array(self.values), dtype=torch.float32, device=device)\n",
    "        return obs, actions, logprobs, rewards, dones, values\n",
    "\n",
    "# GAE advantage computation\n",
    "def compute_gae(rewards, dones, values, last_value, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards, dones, values: shape (T, N)\n",
    "    last_value: shape (N,)\n",
    "    Everything on the correct device.\n",
    "    \"\"\"\n",
    "    T, N = rewards.shape\n",
    "    advantages = torch.zeros(T, N, device=device)\n",
    "    gae = torch.zeros(N, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        mask = 1.0 - dones[t]  # 0 se done, 1 altrimenti\n",
    "        next_value = last_value if t == T - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_value * mask - values[t]\n",
    "        gae = delta + gamma * lam * mask * gae\n",
    "        advantages[t] = gae\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n",
    "\n",
    "# PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_size=7,\n",
    "        n_actions=4,\n",
    "        lr=4e-4,\n",
    "        gamma=0.99,\n",
    "        lam=0.95,\n",
    "        clip_eps=0.2,\n",
    "        epochs=4,\n",
    "        batch_size=1024,\n",
    "        ent_coef=0.05,   \n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_eps = clip_eps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.ent_coef = ent_coef\n",
    "\n",
    "        self.net = PPOActorCritic(board_size=board_size, n_channels=4, n_actions=n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def update(self, buffer: RolloutBuffer, last_value):\n",
    "        # Move last_value to device\n",
    "        last_value = last_value.to(device)\n",
    "\n",
    "        obs, actions, old_logprobs, rewards, dones, values = buffer.to_tensors()\n",
    "\n",
    "        # obs: (T, N, H, W, C)\n",
    "        T, N = rewards.shape\n",
    "        obs = obs.view(T * N, *obs.shape[2:])\n",
    "        actions = actions.view(T * N)\n",
    "        old_logprobs = old_logprobs.view(T * N)\n",
    "        rewards = rewards.view(T, N)\n",
    "        dones = dones.view(T, N)\n",
    "        values = values.view(T, N)\n",
    "        last_value = last_value.view(N)\n",
    "\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards, dones, values, last_value,\n",
    "            gamma=self.gamma, lam=self.lam\n",
    "        )\n",
    "\n",
    "        advantages = advantages.view(T * N)\n",
    "        returns = returns.view(T * N)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # PPO update\n",
    "        dataset_size = T * N\n",
    "        idxs = np.arange(dataset_size)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_idx = idxs[start:end]\n",
    "\n",
    "                batch_obs = obs[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_logprobs = old_logprobs[batch_idx]\n",
    "                batch_adv = advantages[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "\n",
    "                logprobs, entropy, values_pred = self.net.evaluate_actions(batch_obs, batch_actions)\n",
    "\n",
    "                ratio = torch.exp(logprobs - batch_old_logprobs)\n",
    "                surr1 = ratio * batch_adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * batch_adv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = F.mse_loss(values_pred, batch_returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - self.ent_coef * entropy.mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "# Training loop PPO\n",
    "def train_ppo(total_steps=2_000_000, n_boards=256, board_size=7, rollout_horizon=256):\n",
    "    env = make_env(n_boards=n_boards, board_size=board_size)\n",
    "    agent = PPOAgent(board_size=board_size)\n",
    "\n",
    "    state = env.to_state()  # (N, H, W, C) NumPy\n",
    "    step_count = 0\n",
    "\n",
    "    reward_history = []\n",
    "    fruits_history = []\n",
    "    wall_deaths_history = []\n",
    "\n",
    "    pbar = tqdm(total=total_steps, desc=\"Training PPO\")\n",
    "    while step_count < total_steps:\n",
    "        buffer = RolloutBuffer()\n",
    "\n",
    "        # Rollout of length T = rollout_horizon\n",
    "        # changing the rollout affects the memory and the discovery of new states\n",
    "        for t in range(rollout_horizon):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            mask_tensor = get_safety_mask(env)\n",
    "            penalty = -1.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, values = agent.net.forward(state_tensor)\n",
    "                logits = logits + penalty * mask_tensor\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample()\n",
    "                logprobs = dist.log_prob(actions)\n",
    "\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "            rewards_tensor = env.move(actions_np)          \n",
    "            rewards_np = rewards_tensor.cpu().numpy().flatten()\n",
    "\n",
    "            next_state = env.to_state()\n",
    "\n",
    "            # done = 1 (HIT_WALL_REWARD)\n",
    "            #dones_np = np.isin(rewards_np, [env.HIT_WALL_REWARD]).astype(np.float32)\n",
    "            dones_np = np.zeros(n_boards, dtype=np.float32)\n",
    "\n",
    "\n",
    "            # metrics\n",
    "            reward_history.append(np.mean(rewards_np))\n",
    "            fruits_history.append(np.sum(rewards_np == env.FRUIT_REWARD))\n",
    "            wall_deaths_history.append(np.sum(rewards_np == env.HIT_WALL_REWARD))\n",
    "\n",
    "            buffer.add(\n",
    "                obs=state,                          # NumPy (N, H, W, C)\n",
    "                action=actions_np.squeeze(-1),      # NumPy (N,)\n",
    "                logprob=logprobs.cpu().numpy(),     # NumPy (N,)\n",
    "                reward=rewards_np,                  # NumPy (N,)\n",
    "                done=dones_np,                      # NumPy (N,)\n",
    "                value=values.cpu().numpy()          # NumPy (N,)\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "            step_count += n_boards\n",
    "            pbar.update(n_boards)\n",
    "\n",
    "            if step_count >= total_steps:\n",
    "                break\n",
    "\n",
    "        # Bootstrap value for last_state\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            _, last_values = agent.net.forward(state_tensor)\n",
    "        agent.update(buffer, last_values)\n",
    "\n",
    "        # Logging\n",
    "        if len(reward_history) > 0 and (step_count // n_boards) % 50 == 0:\n",
    "            print(\n",
    "                f\"Steps: {step_count} | \"\n",
    "                f\"Avg reward last 500: {np.mean(reward_history[-500:]):.3f} | \"\n",
    "                f\"Fruits last 500: {np.mean(fruits_history[-500:]):.2f} | \"\n",
    "                f\"Wall deaths last 500: {np.mean(wall_deaths_history[-500:]):.2f}\"\n",
    "            )\n",
    "\n",
    "    pbar.close()\n",
    "    return agent, reward_history, fruits_history, wall_deaths_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PPO agent on NUM_BOARDS parallel boards\n",
    "\n",
    "# Good with rollout_horizon=256 and entropy_coeff=0.05\n",
    "\n",
    "agent, rew_hist, fruits_hist, wall_hist = train_ppo(\n",
    "    total_steps=5_000_000,\n",
    "    n_boards=NUM_BOARDS,\n",
    "    board_size=7,\n",
    "    rollout_horizon=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained weights\n",
    "torch.save(agent.net.state_dict(), \"ppo_snake_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712044b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 200\n",
    "kernel = np.ones(window) / window\n",
    "\n",
    "# Moving averages\n",
    "rew_moving = np.convolve(rew_hist, kernel, mode='valid')\n",
    "fruits_moving = np.convolve(fruits_hist, kernel, mode='valid')\n",
    "deaths_moving = np.convolve(wall_hist, kernel, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Average reward\n",
    "# -------------------------------\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(rew_hist, color='gray', alpha=0.15, linewidth=1, label=\"Raw\")\n",
    "plt.plot(rew_moving, color='black', linewidth=2.5, label=\"Moving Avg\")\n",
    "plt.title(\"Average Reward per Rollout\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Fruits eaten (total)\n",
    "# -------------------------------\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(fruits_hist, color='green', alpha=0.15, linewidth=1, label=\"Raw\")\n",
    "plt.plot(fruits_moving, color='green', linewidth=2.5, label=\"Moving Avg\")\n",
    "plt.title(\"Fruits Eaten per Rollout\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Total Fruits\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Wall deaths (total)\n",
    "# -------------------------------\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(wall_hist, color='red', alpha=0.15, linewidth=1, label=\"Raw\")\n",
    "plt.plot(deaths_moving, color='red', linewidth=2.5, label=\"Moving Avg\")\n",
    "plt.title(\"Wall Deaths per Rollout\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Total Deaths\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b85098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_policy(env):\n",
    "    n = env.n_boards\n",
    "    actions = np.zeros((n, 1), dtype=np.int32)\n",
    "\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    fruits = np.argwhere(env.boards == env.FRUIT)\n",
    "\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "    fruits = fruits[fruits[:, 0].argsort()]\n",
    "\n",
    "    mask_all = get_safety_mask(env).cpu().numpy()\n",
    "\n",
    "    # moves: 0=down,1=right,2=up,3=left\n",
    "    for i in range(n):\n",
    "        b, hx, hy = heads[i]\n",
    "        _, fx, fy = fruits[i]\n",
    "\n",
    "        # greedy preferences\n",
    "        preferred = []\n",
    "        if fx > hx: preferred.append(0)\n",
    "        if fx < hx: preferred.append(2)\n",
    "        if fy > hy: preferred.append(1)\n",
    "        if fy < hy: preferred.append(3)\n",
    "\n",
    "        mask = mask_all[i]\n",
    "\n",
    "        # 1. Try greedy safe moves\n",
    "        safe_preferred = [m for m in preferred if mask[m] == 0]\n",
    "        if safe_preferred:\n",
    "            actions[i] = random.choice(safe_preferred)\n",
    "            continue\n",
    "\n",
    "        # 2. Try any safe move\n",
    "        safe_moves = [m for m in range(4) if mask[m] == 0]\n",
    "        if safe_moves:\n",
    "            actions[i] = random.choice(safe_moves)\n",
    "            continue\n",
    "\n",
    "        # 3. No safe moves ‚Üí choose randomly (like PPO can)\n",
    "        actions[i] = np.random.randint(0, 4)\n",
    "\n",
    "    return actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa027fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To assess the performance after training, we evaluate the baseline and the PPO agent on 1000 steps and compare the average reward, fruits, and deaths. \n",
    "\n",
    "def evaluate_baseline(steps=1000):\n",
    "    env = get_env(n=NUM_BOARDS)\n",
    "    fruits = []\n",
    "    deaths = []\n",
    "    reward_means = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        actions = baseline_policy(env)\n",
    "        rewards = env.move(actions)\n",
    "        rewards = rewards.numpy().flatten() if hasattr(rewards, \"numpy\") else rewards\n",
    "\n",
    "        # metriche\n",
    "        fruits.append(np.sum(rewards == env.FRUIT_REWARD))\n",
    "        deaths.append(np.sum(rewards == env.HIT_WALL_REWARD))\n",
    "        reward_means.append(np.mean(rewards))\n",
    "\n",
    "    return (\n",
    "        np.array(fruits),\n",
    "        np.array(deaths),\n",
    "        np.array(reward_means)\n",
    "    )\n",
    "\n",
    "baseline_fruits, baseline_deaths, baseline_reward = evaluate_baseline(steps=1000)\n",
    "baseline_reward_avg = np.mean(baseline_reward)\n",
    "baseline_fruits_avg = np.mean(baseline_fruits)\n",
    "baseline_deaths_avg = np.mean(baseline_deaths)\n",
    "\n",
    "def evaluate_agent(agent, steps=1000):\n",
    "    env = get_env(n=NUM_BOARDS)\n",
    "    fruits = []\n",
    "    deaths = []\n",
    "    rewards_mean = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        state = env.to_state()\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _ = agent.net(state_tensor)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            actions = dist.sample().cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "        rewards = env.move(actions).cpu().numpy().flatten()\n",
    "\n",
    "        fruits.append(np.sum(rewards == env.FRUIT_REWARD))\n",
    "        deaths.append(np.sum(rewards == env.HIT_WALL_REWARD))\n",
    "        rewards_mean.append(np.mean(rewards))\n",
    "\n",
    "    return (\n",
    "        np.array(fruits),\n",
    "        np.array(deaths),\n",
    "        np.array(rewards_mean)\n",
    "    )\n",
    "\n",
    "ppo_fruits_eval, ppo_deaths_eval, ppo_reward_eval = evaluate_agent(agent, steps=1000)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\"Reward per Step\", \"Fruits per Step\", \"Deaths per Step\"],\n",
    "    \"PPO\": [\n",
    "        np.mean(ppo_reward_eval),\n",
    "        np.mean(ppo_fruits_eval),\n",
    "        np.mean(ppo_deaths_eval)\n",
    "    ],\n",
    "    \"Baseline\": [\n",
    "        baseline_reward_avg,\n",
    "        baseline_fruits_avg,\n",
    "        baseline_deaths_avg\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffedae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION DURING TRAINING OF PPO AGENT \n",
    "# We just consider the rollouts during training because they are already collected and give a good idea of the learning curve. \n",
    "# We compute the moving average of reward, fruits, and deaths during training and compare them with the baseline averages.\n",
    "# Baseline evaluation is done in the above cell.\n",
    "\n",
    "window = 200\n",
    "kernel = np.ones(window) / window\n",
    "\n",
    "# Moving averages PPO\n",
    "reward_moving = np.convolve(rew_hist, kernel, mode='valid')\n",
    "fruits_moving = np.convolve(fruits_hist, kernel, mode='valid')\n",
    "deaths_moving = np.convolve(wall_hist, kernel, mode='valid')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# Reward per step\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(rew_hist, color='black', alpha=0.15, linewidth=1, label=\"Raw Data\")\n",
    "plt.plot(reward_moving, color='black', linewidth=2.5, label=\"PPO Moving Avg\")\n",
    "plt.axhline(y=baseline_reward_avg, color='orange', linestyle='--', linewidth=2,\n",
    "            label=f\"Baseline Reward\")\n",
    "plt.title(\"Reward per Step\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Fruits per step\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(fruits_hist, color='green', alpha=0.15, linewidth=1, label=\"Raw Data\")\n",
    "plt.plot(fruits_moving, color='green', linewidth=2.5, label=\"PPO Moving Avg\")\n",
    "plt.axhline(y=baseline_fruits_avg, color='orange', linestyle='--', linewidth=2,\n",
    "            label=f\"Baseline Fruits\")\n",
    "plt.title(\"Fruits per Step\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Fruits\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Wall deaths per step\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(wall_hist, color='red', alpha=0.15, linewidth=1, label=\"Raw Data\")\n",
    "plt.plot(deaths_moving, color='red', linewidth=2.5, label=\"PPO Moving Avg\")\n",
    "plt.axhline(y=baseline_deaths_avg, color='orange', linestyle='--', linewidth=2,\n",
    "            label=f\"Baseline Deaths\")\n",
    "plt.title(\"Wall Deaths per Step\")\n",
    "plt.xlabel(\"Rollout Iteration\")\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362c8fa",
   "metadata": {},
   "source": [
    "### Play Snake with PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8a83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "agent = PPOAgent(board_size=7, n_actions=4)\n",
    "agent.net.load_state_dict(torch.load(\"ppo_snake_weights.pth\"))\n",
    "agent.net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae990465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_game(model, max_steps=100):\n",
    "    # Create a single environment for visualization\n",
    "    game_env = get_env(n=1)\n",
    "    state = game_env.to_state()\n",
    "    frames = []\n",
    "\n",
    "    frames.append(game_env.boards[0].copy())\n",
    "\n",
    "    print(\"Generating animation...\", end=\"\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Safety mask\n",
    "        mask = get_safety_mask(game_env)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, _ = model(state_tensor)\n",
    "            logits = logits + mask * -1.0\n",
    "            actions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "        game_env.move(actions_np)\n",
    "        state = game_env.to_state()\n",
    "\n",
    "        frames.append(game_env.boards[0].copy())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 1. STATIC SNAPSHOTS EVERY 15 STEPS\n",
    "    # -----------------------------------------\n",
    "    snapshot_steps = [0, 10, 20, 30, 40]\n",
    "    snapshot_steps = [s for s in snapshot_steps if s < len(frames)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(snapshot_steps), figsize=(15, 3))\n",
    "\n",
    "    for ax, step in zip(axes, snapshot_steps):\n",
    "        ax.imshow(frames[step], origin='lower', cmap='viridis', vmin=0, vmax=4)\n",
    "        ax.set_title(f\"Step {step}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 2. ANIMATION\n",
    "    # -----------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "\n",
    "    img = ax.imshow(frames[0], origin='lower', cmap='viridis', vmin=0, vmax=4)\n",
    "    ax.set_title(\"Snake Agent Replay\")\n",
    "\n",
    "    def update(frame):\n",
    "        img.set_array(frame)\n",
    "        return [img]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=frames,\n",
    "        interval=100,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "    return HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_game(agent.net, max_steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl-env)",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
