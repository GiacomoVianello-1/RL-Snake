{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473c0227",
   "metadata": {},
   "source": [
    "# Snake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ccdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environments\n",
    "import environments_fully_observable \n",
    "import environments_partially_observable\n",
    "\n",
    "# Utilities\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Check for GPU availability (CUDA for Nvidia, MPS for Mac M4)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"✅ GPU Found: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"✅ MPS (Metal Performance Shaders) Found: Apple Silicon Acceleration enabled.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"❌ No GPU Found. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894046e",
   "metadata": {},
   "source": [
    "## Environment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# function to standardize getting an env for the whole notebook\n",
    "def get_env(n=1000):\n",
    "    # n is the number of boards that you want to simulate parallely\n",
    "    # size is the size of each board, also considering the borders\n",
    "    # mask for the partially observable, is the size of the local neighborhood\n",
    "    size = 7\n",
    "    e = environments_fully_observable.OriginalSnakeEnvironment(n, size)\n",
    "    # or environments_partially_observable.OriginalSnakeEnvironment(n, size, 2)\n",
    "    return e\n",
    "\n",
    "env_ = get_env()\n",
    "\n",
    "fig, axs = plt.subplots(1, min(len(env_.boards), 5), figsize=(10,3))\n",
    "for ax, board in zip(axs, env_.boards):\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.imshow(board, origin=\"lower\")\n",
    "    \n",
    "\n",
    "# Useful for future experiments if we want to change the number of (parallel) boards and their size \n",
    "# in the training loop without having to change the environment code\n",
    "\n",
    "NUM_BOARDS = 1000\n",
    "BOARD_SIZE = 7\n",
    "ITERATIONS = 20_000_000   # Total training steps (a lot, but we use torch and GPU support for this reason!)\n",
    "\n",
    "# Full Environment wrapper\n",
    "def make_env(n_boards, board_size):\n",
    "    return environments_fully_observable.OriginalSnakeEnvironment(n_boards,board_size)\n",
    "\n",
    "# Safety mask fuction\n",
    "def get_safety_mask(env):\n",
    "    '''\n",
    "    For each parallel board, identifies the current coordinates of the snake's head and calculates \n",
    "    the potential next position for each of the four cardinal moves: UP, RIGHT, DOWN, and LEFT. \n",
    "    If a projected move targets a cell occupied by a WALL (represented by value 0), \n",
    "    the corresponding action is flagged as \"unsafe\" in a boolean mask.\n",
    "    '''\n",
    "    masks = np.zeros((env.n_boards, 4), dtype=np.float32)\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "\n",
    "    moves = {0:(1,0), 1:(0,1), 2:(-1,0), 3:(0,-1)}\n",
    "\n",
    "    for b, hx, hy in heads:\n",
    "        for a, (dx, dy) in moves.items():\n",
    "            nx, ny = hx+dx, hy+dy\n",
    "            if env.boards[b, nx, ny] == env.WALL:\n",
    "                masks[b, a] = 1.0   # unsafe for wall\n",
    "    return torch.tensor(masks, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f130f31",
   "metadata": {},
   "source": [
    "# Models and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c35a97",
   "metadata": {},
   "source": [
    "## PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89346df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PPO Actor-Critic Network\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, board_size=7, n_channels=4, n_actions=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(n_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        self.board_size = board_size\n",
    "        conv_out_dim = 128 * board_size * board_size\n",
    "\n",
    "        self.fc = nn.Linear(conv_out_dim, 128)\n",
    "        self.policy_head = nn.Linear(128, n_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = x.permute(0, 3, 1, 2)          # (N, H, W, C) -> (N, C, H, W)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "    def act(self, x, mask=None):\n",
    "        logits, value = self.forward(x)\n",
    "        if mask is not None:\n",
    "            logits = logits + mask\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        logprob = dist.log_prob(action)\n",
    "        return action, logprob, value\n",
    "\n",
    "\n",
    "    def evaluate_actions(self, x, actions):\n",
    "        logits, values = self.forward(x)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        logprobs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy()\n",
    "        return logprobs, entropy, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35925724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rollout Buffer for PPO\n",
    "@dataclass\n",
    "class RolloutBuffer:\n",
    "    obs: list\n",
    "    actions: list\n",
    "    logprobs: list\n",
    "    rewards: list\n",
    "    values: list\n",
    "\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "\n",
    "    def add(self, obs, action, logprob, reward, value):\n",
    "        # obs, reward, value, logprob, action are numpy or scalars\n",
    "        self.obs.append(obs)\n",
    "        self.actions.append(action)\n",
    "        self.logprobs.append(logprob)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "\n",
    "    def to_tensors(self):\n",
    "        # conver all to torch tensors\n",
    "        obs = torch.tensor(np.array(self.obs), dtype=torch.float32, device=device)\n",
    "        actions = torch.tensor(np.array(self.actions), dtype=torch.long, device=device)\n",
    "        logprobs = torch.tensor(np.array(self.logprobs), dtype=torch.float32, device=device)\n",
    "        rewards = torch.tensor(np.array(self.rewards), dtype=torch.float32, device=device)\n",
    "        values = torch.tensor(np.array(self.values), dtype=torch.float32, device=device)\n",
    "        return obs, actions, logprobs, rewards, values\n",
    "\n",
    "# Define the GAE computation\n",
    "def compute_gae(rewards, values, last_value, gamma=0.99, lam=0.95):\n",
    "    T, N = rewards.shape\n",
    "    advantages = torch.zeros(T, N, device=device)\n",
    "    gae = torch.zeros(N, device=device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        next_value = last_value if t == T - 1 else values[t + 1]\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "\n",
    "    returns = advantages + values\n",
    "    return advantages, returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9de8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PPO Agent class:\n",
    "class PPOAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        board_size=7,\n",
    "        n_actions=4,\n",
    "        lr=4e-4,\n",
    "        gamma=0.995,\n",
    "        lam=0.95,\n",
    "        clip_eps=0.2,\n",
    "        epochs=4,\n",
    "        batch_size=1024,\n",
    "        ent_coef=0.05,   \n",
    "    ):\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.clip_eps = clip_eps\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.ent_coef = ent_coef\n",
    "        self.ent_coef_initial = ent_coef\n",
    "\n",
    "        self.net = PPOActorCritic(board_size=board_size, n_channels=4, n_actions=n_actions).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    def update(self, buffer: RolloutBuffer, last_value):\n",
    "        # Move last_value to device\n",
    "        last_value = last_value.to(device)\n",
    "\n",
    "        obs, actions, old_logprobs, rewards, values = buffer.to_tensors()\n",
    "\n",
    "        # obs: (T, N, H, W, C)\n",
    "        T, N = rewards.shape\n",
    "        obs = obs.view(T * N, *obs.shape[2:])\n",
    "        actions = actions.view(T * N)\n",
    "        old_logprobs = old_logprobs.view(T * N)\n",
    "        rewards = rewards.view(T, N)\n",
    "        values = values.view(T, N)\n",
    "        last_value = last_value.view(N)\n",
    "\n",
    "        advantages, returns = compute_gae(\n",
    "            rewards, values, last_value,\n",
    "            gamma=self.gamma, lam=self.lam\n",
    "        )\n",
    "\n",
    "        advantages = advantages.view(T * N)\n",
    "        returns = returns.view(T * N)\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # PPO update\n",
    "        dataset_size = T * N\n",
    "        idxs = np.arange(dataset_size)\n",
    "\n",
    "        total_policy_loss = 0.0\n",
    "        total_value_loss = 0.0\n",
    "        total_entropy = 0.0\n",
    "        total_loss = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            np.random.shuffle(idxs)\n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_idx = idxs[start:end]\n",
    "\n",
    "                # we select the batch and compute the losses\n",
    "                batch_obs = obs[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_logprobs = old_logprobs[batch_idx]\n",
    "                batch_adv = advantages[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "\n",
    "                logprobs, entropy, values_pred = self.net.evaluate_actions(batch_obs, batch_actions)\n",
    "\n",
    "                ratio = torch.exp(logprobs - batch_old_logprobs)\n",
    "                surr1 = ratio * batch_adv\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * batch_adv\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = F.mse_loss(values_pred, batch_returns)\n",
    "\n",
    "                loss = policy_loss + 0.5 * value_loss - self.ent_coef * entropy.mean()\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.net.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # accumulate \n",
    "                total_policy_loss += policy_loss.item() \n",
    "                total_value_loss += value_loss.item() \n",
    "                total_entropy += entropy.mean().item() \n",
    "                total_loss += loss.item() \n",
    "                batches += 1\n",
    "        return { \n",
    "            \"policy_loss\": total_policy_loss / batches, \n",
    "            \"value_loss\": total_value_loss / batches, \n",
    "            \"entropy\": total_entropy / batches, \n",
    "            \"total_loss\": total_loss / batches, \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop PPO\n",
    "def train_ppo(total_steps=2_000_000, n_boards=256, board_size=7, rollout_horizon=256):\n",
    "    env = make_env(n_boards=n_boards, board_size=board_size)\n",
    "    agent = PPOAgent(board_size=board_size)\n",
    "\n",
    "    state = env.to_state()\n",
    "    step_count = 0\n",
    "\n",
    "    reward_history = []\n",
    "    fruits_history = []\n",
    "    wall_deaths_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    last_log_step = 0\n",
    "    log_interval = 500000\n",
    "\n",
    "    pbar = tqdm(total=total_steps, desc=\"Training PPO\")\n",
    "    while step_count < total_steps:\n",
    "        buffer = RolloutBuffer()\n",
    "\n",
    "        # Rollout collection\n",
    "        for t in range(rollout_horizon):\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "            mask_tensor = get_safety_mask(env)\n",
    "            penalty = -1.0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits, values = agent.net.forward(state_tensor)\n",
    "                logits = logits + penalty * mask_tensor                 # Soft Mask\n",
    "                #logits = logits.masked_fill(mask_tensor == 1, -1e9)    # Hard Mask\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample()\n",
    "                logprobs = dist.log_prob(actions)\n",
    "\n",
    "            actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "            rewards_tensor = env.move(actions_np)\n",
    "            rewards_np = rewards_tensor.cpu().numpy().flatten()\n",
    "\n",
    "            next_state = env.to_state()\n",
    "\n",
    "            # metrics\n",
    "            reward_history.append(np.mean(rewards_np))\n",
    "            fruits_history.append(np.sum(rewards_np == env.FRUIT_REWARD))\n",
    "            wall_deaths_history.append(np.sum(rewards_np == env.HIT_WALL_REWARD))\n",
    "\n",
    "            buffer.add(\n",
    "                obs=state,\n",
    "                action=actions_np.squeeze(-1),\n",
    "                logprob=logprobs.cpu().numpy(),\n",
    "                reward=rewards_np,\n",
    "                value=values.cpu().numpy()\n",
    "            )\n",
    "\n",
    "            state = next_state\n",
    "            step_count += n_boards\n",
    "            pbar.update(n_boards)\n",
    "\n",
    "            if step_count >= total_steps:\n",
    "                break\n",
    "\n",
    "        # PPO update\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            _, last_values = agent.net.forward(state_tensor)\n",
    "\n",
    "        loss_dict = agent.update(buffer, last_values) \n",
    "        loss_history.append(loss_dict)\n",
    "\n",
    "        # Entropy decay\n",
    "        progress = step_count / total_steps\n",
    "        agent.ent_coef = agent.ent_coef_initial * (1 - progress)\n",
    "\n",
    "        # Logging every 50k steps\n",
    "        if step_count - last_log_step >= log_interval:\n",
    "            last_log_step = step_count\n",
    "\n",
    "            avg_reward = np.mean(reward_history[-500:])\n",
    "            avg_fruits = np.mean(fruits_history[-500:])\n",
    "            avg_deaths = np.mean(wall_deaths_history[-500:])\n",
    "\n",
    "            avg_policy_loss = np.mean([l[\"policy_loss\"] for l in loss_history[-50:]])\n",
    "            avg_value_loss = np.mean([l[\"value_loss\"] for l in loss_history[-50:]])\n",
    "            avg_entropy = np.mean([l[\"entropy\"] for l in loss_history[-50:]])\n",
    "            avg_total_loss = np.mean([l[\"total_loss\"] for l in loss_history[-50:]])\n",
    "\n",
    "            tqdm.write(\n",
    "                f\"\\nSteps: {step_count:,}\"\n",
    "                f\"\\n  Reward (last 500): {avg_reward:.3f}\"\n",
    "                f\"\\n  Fruits (last 500): {avg_fruits:.2f}\"\n",
    "                f\"\\n  Deaths (last 500): {avg_deaths:.2f}\"\n",
    "                f\"\\n  Policy Loss (avg 50 updates): {avg_policy_loss:.4f}\"\n",
    "                f\"\\n  Value Loss  (avg 50 updates): {avg_value_loss:.4f}\"\n",
    "                f\"\\n  Entropy     (avg 50 updates): {avg_entropy:.4f}\"\n",
    "                f\"\\n  Total Loss  (avg 50 updates): {avg_total_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "    pbar.close()\n",
    "    return agent, reward_history, fruits_history, wall_deaths_history, loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c4bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the PPO agent on NUM_BOARDS parallel boards\n",
    "ppo_agent, ppo_reward_hist, ppo_fruits_hist, ppo_wall_hist, ppo_loss_hist = train_ppo(\n",
    "    total_steps=ITERATIONS, \n",
    "    n_boards=NUM_BOARDS,    # best 1000\n",
    "    board_size=BOARD_SIZE,\n",
    "    rollout_horizon=256     # best 256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf862c",
   "metadata": {},
   "source": [
    "## A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantage Actor-Critic (A2C) Agent\n",
    "class A2CNet(nn.Module):\n",
    "    def __init__(self, board_size=7, n_channels=4, n_actions=4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        self.board_size = board_size\n",
    "        conv_out_dim = 128 * board_size * board_size\n",
    "        self.fc = nn.Linear(conv_out_dim, 128)\n",
    "        self.policy_head = nn.Linear(128, n_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,3,1,2)  # NHWC --> NCHW\n",
    "\n",
    "        # Feature extraction\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and dense processing\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        # Output heads\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x).squeeze(-1)\n",
    "        \n",
    "        return logits, value\n",
    "\n",
    "\n",
    "def train_a2c(total_steps=2_000_000, n_boards=NUM_BOARDS, board_size=7):\n",
    "    env = make_env(n_boards=n_boards, board_size=board_size)\n",
    "    net = A2CNet(board_size=board_size).to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4)\n",
    "\n",
    "    state = env.to_state()\n",
    "    step_count = 0\n",
    "    pbar = tqdm(total=total_steps, desc=\"Training A2C\")\n",
    "\n",
    "    gamma = 0.995\n",
    "\n",
    "    # Logging buffers\n",
    "    reward_history = []\n",
    "    fruits_history = []\n",
    "    deaths_history = []\n",
    "    policy_loss_hist = []\n",
    "    value_loss_hist = []\n",
    "    entropy_hist = []\n",
    "\n",
    "    last_log_step = 0\n",
    "    log_interval = 750_000\n",
    "\n",
    "    while step_count < total_steps:\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits, v_s = net(s)\n",
    "\n",
    "        # Safety Mask\n",
    "        mask = get_safety_mask(env)\n",
    "        mask_tensor = mask.to(device)\n",
    "        penalty = -1\n",
    "        logits = logits + penalty * mask_tensor\n",
    "\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        a = dist.sample()\n",
    "\n",
    "        # Step environment\n",
    "        rewards = env.move(a.cpu().numpy().reshape(-1,1))\n",
    "        r = rewards.cpu().numpy().flatten()\n",
    "        next_state = env.to_state()\n",
    "\n",
    "        # Bootstrap V(s')\n",
    "        with torch.no_grad():\n",
    "            s_next = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            _, v_next = net(s_next)\n",
    "\n",
    "        # TD error δ = r + γV(s') − V(s)\n",
    "        r_t = torch.tensor(r, dtype=torch.float32, device=device)\n",
    "        delta = r_t + gamma * v_next - v_s\n",
    "\n",
    "        # Losses\n",
    "        logprobs = dist.log_prob(a)\n",
    "        policy_loss = -(logprobs * delta.detach()).mean()\n",
    "        value_loss = delta.pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.05 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "        step_count += n_boards\n",
    "        pbar.update(n_boards)\n",
    "\n",
    "        # Store metrics\n",
    "        reward_history.append(np.mean(r))\n",
    "        fruits_history.append(np.sum(r == env.FRUIT_REWARD))\n",
    "        deaths_history.append(np.sum(r == env.HIT_WALL_REWARD))\n",
    "        policy_loss_hist.append(policy_loss.item())\n",
    "        value_loss_hist.append(value_loss.item())\n",
    "        entropy_hist.append(entropy.item())\n",
    "\n",
    "        # Periodic logging\n",
    "        if step_count - last_log_step >= log_interval:\n",
    "            last_log_step = step_count\n",
    "            tqdm.write(\n",
    "                f\"\\nSteps: {step_count:,}\"\n",
    "                f\"\\n  Reward (last 500): {np.mean(reward_history[-500:]):.3f}\"\n",
    "                f\"\\n  Fruits (last 500): {np.mean(fruits_history[-500:]):.2f}\"\n",
    "                f\"\\n  Deaths (last 500): {np.mean(deaths_history[-500:]):.2f}\"\n",
    "                f\"\\n  Policy Loss (last 500): {np.mean(policy_loss_hist[-500:]):.4f}\"\n",
    "                f\"\\n  Value Loss  (last 500): {np.mean(value_loss_hist[-500:]):.4f}\"\n",
    "                f\"\\n  Entropy     (last 500): {np.mean(entropy_hist[-500:]):.4f}\"\n",
    "            )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    return net, reward_history, fruits_history, deaths_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00844c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train A2C agent\n",
    "a2c_agent, a2c_reward_hist, a2c_fruits_hist, a2c_wall_hist = train_a2c(\n",
    "    total_steps=ITERATIONS,\n",
    "    n_boards=NUM_BOARDS,\n",
    "    board_size=BOARD_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a91e30",
   "metadata": {},
   "source": [
    "## DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5060f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Agent\n",
    "class DDQNNet(nn.Module):\n",
    "    def __init__(self, board_size=7, n_channels=4, n_actions=4):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "\n",
    "        self.board_size = board_size\n",
    "        conv_out_dim = 128 * board_size * board_size\n",
    "\n",
    "        # Intermediate fully connected layer for non-linear feature refinement\n",
    "        self.fc = nn.Linear(conv_out_dim, 128)\n",
    "\n",
    "        # Q-value head: outputs an estimated value for each action\n",
    "        self.q_head = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with ReLU activations\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Flatten and process through the dense layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "\n",
    "        # Output the Q-values for each of the 4 actions\n",
    "        return self.q_head(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100_000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, s, a, r, s_next):\n",
    "        self.buffer.append((s, a, r, s_next))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        s, a, r, s_next = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(np.array(s), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.array(a), dtype=torch.long, device=device),\n",
    "            torch.tensor(np.array(r), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.array(s_next), dtype=torch.float32, device=device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "def train_ddqn(\n",
    "    total_steps=2_000_000,\n",
    "    n_boards=NUM_BOARDS,\n",
    "    board_size=7,\n",
    "    batch_size=256,\n",
    "    gamma=0.995,\n",
    "    lr=4e-4,\n",
    "    start_learning=10_000,\n",
    "    target_update_interval=5_000,\n",
    "    eps_start=0.5,\n",
    "    eps_end=0.01,\n",
    "):\n",
    "    env = make_env(n_boards=n_boards, board_size=board_size)\n",
    "    online = DDQNNet(board_size=board_size).to(device)\n",
    "    target = DDQNNet(board_size=board_size).to(device)\n",
    "    target.load_state_dict(online.state_dict())\n",
    "    target.eval()\n",
    "\n",
    "    optimizer = optim.Adam(online.parameters(), lr=lr)\n",
    "    buffer = ReplayBuffer()\n",
    "\n",
    "    state = env.to_state()\n",
    "    step_count = 0\n",
    "    pbar = tqdm(total=total_steps, desc=\"Training DDQN\")\n",
    "\n",
    "    rew_hist, fruits_hist, deaths_hist = [], [], []\n",
    "\n",
    "    last_log_step = 0\n",
    "    log_interval = 500_000\n",
    "\n",
    "    # Exponential epsilon decay\n",
    "    DECAY_STEPS = int(ITERATIONS * (2/3))\n",
    "    tau = DECAY_STEPS / 3.0\n",
    "\n",
    "    def epsilon_by_step(t):\n",
    "        return eps_end + (eps_start - eps_end) * np.exp(-t / tau)\n",
    "\n",
    "    while step_count < total_steps:\n",
    "        s = state  # (N, H, W, C)\n",
    "        eps = epsilon_by_step(step_count)\n",
    "\n",
    "        # Epsilon-greedy\n",
    "        with torch.no_grad():\n",
    "            s_tensor = torch.tensor(s, dtype=torch.float32, device=device).permute(0, 3, 1, 2) # (N, C, H, W)\n",
    "            q_values = online(s_tensor) # (N, 4)\n",
    "\n",
    "            # Safety mask\n",
    "            mask = get_safety_mask(env)\n",
    "            penalty = -1\n",
    "            q_values = q_values + penalty * mask\n",
    "\n",
    "            greedy_actions = torch.argmax(q_values, dim=-1).cpu().numpy()\n",
    "\n",
    "        random_mask = np.random.rand(n_boards) < eps\n",
    "        random_actions = np.random.randint(0, 4, size=n_boards)\n",
    "        actions = np.where(random_mask, random_actions, greedy_actions).reshape(-1, 1)\n",
    "\n",
    "        rewards = env.move(actions)\n",
    "        r = rewards.cpu().numpy().flatten()\n",
    "        next_state = env.to_state()\n",
    "\n",
    "        # logging\n",
    "        rew_hist.append(np.mean(r))\n",
    "        fruits_hist.append(np.sum(r == env.FRUIT_REWARD))\n",
    "        deaths_hist.append(np.sum(r == env.HIT_WALL_REWARD))\n",
    "\n",
    "        # push transitions (per board) in buffer\n",
    "        for i in range(n_boards):\n",
    "            buffer.push(s[i], actions[i, 0], r[i], next_state[i])\n",
    "\n",
    "        state = next_state\n",
    "        step_count += n_boards\n",
    "        pbar.update(n_boards)\n",
    "\n",
    "        # learning\n",
    "        if len(buffer) >= start_learning:\n",
    "            s_b, a_b, r_b, s_next_b = buffer.sample(batch_size) # (B, H, W, C)\n",
    "            \n",
    "            s_b = s_b.permute(0, 3, 1, 2) # (B, C, H, W)\n",
    "            s_next_b = s_next_b.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Q(s,a)\n",
    "            q = online(s_b).gather(1, a_b.view(-1, 1)).squeeze(1)\n",
    "\n",
    "            # Double DQN target\n",
    "            with torch.no_grad():\n",
    "                online_next = online(s_next_b)\n",
    "                best_actions = torch.argmax(online_next, dim=-1, keepdim=True)\n",
    "                target_next = target(s_next_b).gather(1, best_actions).squeeze(1)\n",
    "                y = r_b + gamma * target_next\n",
    "\n",
    "            loss = F.mse_loss(q, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # target update\n",
    "        if step_count % target_update_interval < n_boards:\n",
    "            target.load_state_dict(online.state_dict())\n",
    "\n",
    "        # logging\n",
    "        if step_count - last_log_step >= log_interval:\n",
    "            last_log_step = step_count\n",
    "            tqdm.write(\n",
    "                f\"\\nSteps: {step_count:,}\"\n",
    "                f\"\\n  Reward (last 500): {np.mean(rew_hist[-500:]):.3f}\"\n",
    "                f\"\\n  Fruits (last 500): {np.mean(fruits_hist[-500:]):.2f}\"\n",
    "                f\"\\n  Deaths (last 500): {np.mean(deaths_hist[-500:]):.2f}\"\n",
    "                f\"\\n  Epsilon: {eps:.3f}\"\n",
    "            )\n",
    "\n",
    "    pbar.close()\n",
    "    return online, rew_hist, fruits_hist, deaths_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d445de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DDQN agent\n",
    "ddqn_agent, ddqn_reward_hist, ddqn_fruits_hist, ddqn_wall_hist = train_ddqn(\n",
    "    total_steps = ITERATIONS,\n",
    "    n_boards = NUM_BOARDS,\n",
    "    board_size = BOARD_SIZE,\n",
    "    eps_start=0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25a99b1",
   "metadata": {},
   "source": [
    "## Training Result Savings\n",
    "\n",
    "**Skip this cell**: \n",
    "This has to be executed only once since it packs all the training results and the weights of the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_results(\n",
    "    algo_name,\n",
    "    model,\n",
    "    reward_hist,\n",
    "    fruits_hist,\n",
    "    deaths_hist,\n",
    "    extra_metrics=None,\n",
    "    params=None,\n",
    "    save_dir=\"results\"\n",
    "):\n",
    "\n",
    "    algo_dir = os.path.join(save_dir, algo_name)\n",
    "    os.makedirs(algo_dir, exist_ok=True)\n",
    "\n",
    "    # Detect the actual PyTorch model inside the agent\n",
    "    if hasattr(model, \"state_dict\"):\n",
    "        # A2C or DDQN (the model itself is a nn.Module)\n",
    "        torch_model = model\n",
    "    elif hasattr(model, \"net\"):\n",
    "        # PPOAgent: the neural network is inside .net\n",
    "        torch_model = model.net\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot find a PyTorch model inside {algo_name} agent\")\n",
    "\n",
    "    # Save weights\n",
    "    torch.save(torch_model.state_dict(), os.path.join(algo_dir, f\"{algo_name}_weights.pt\"))\n",
    "\n",
    "    # Save training data\n",
    "    data = {\n",
    "        \"reward_hist\": reward_hist,\n",
    "        \"fruits_hist\": fruits_hist,\n",
    "        \"deaths_hist\": deaths_hist,\n",
    "        \"params\": params or {},\n",
    "    }\n",
    "\n",
    "    if extra_metrics is not None:\n",
    "        data.update(extra_metrics)\n",
    "\n",
    "    with open(os.path.join(algo_dir, f\"{algo_name}_training.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    print(f\"Saved {algo_name} results in {algo_dir}\")\n",
    "\n",
    "# PPO\n",
    "params = {\n",
    "    \"lr\": 3e-4,\n",
    "    \"gamma\": 0.99,\n",
    "    \"n_boards\": NUM_BOARDS,\n",
    "    \"board_size\": BOARD_SIZE,\n",
    "    \"total_steps\": ITERATIONS,\n",
    "}\n",
    "save_training_results(\n",
    "    algo_name=\"ppo\",\n",
    "    model=ppo_agent,\n",
    "    reward_hist=ppo_reward_hist,\n",
    "    fruits_hist=ppo_fruits_hist,\n",
    "    deaths_hist=ppo_wall_hist,\n",
    "    extra_metrics={\"loss_hist\": ppo_loss_hist},\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# A2C\n",
    "save_training_results(\n",
    "    algo_name=\"a2c\",\n",
    "    model=a2c_agent,\n",
    "    reward_hist=a2c_reward_hist,\n",
    "    fruits_hist=a2c_fruits_hist,\n",
    "    deaths_hist=a2c_wall_hist,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# DDQN\n",
    "save_training_results(\n",
    "    algo_name=\"ddqn\",\n",
    "    model=ddqn_agent,\n",
    "    reward_hist=ddqn_reward_hist,\n",
    "    fruits_hist=ddqn_fruits_hist,\n",
    "    deaths_hist=ddqn_wall_hist,\n",
    "    params=params\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97f7f7",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASELINE POLICY\n",
    "# Acts greedily toward the fruit and checks for walls and body. In case of no safe moves, it chooses randomly among all possible actions. \n",
    "\n",
    "def baseline_policy(env):\n",
    "    n = env.n_boards\n",
    "    actions = np.zeros((n, 1), dtype=np.int32)\n",
    "\n",
    "    heads = np.argwhere(env.boards == env.HEAD)\n",
    "    fruits = np.argwhere(env.boards == env.FRUIT)\n",
    "\n",
    "    heads = heads[heads[:, 0].argsort()]\n",
    "    fruits = fruits[fruits[:, 0].argsort()]\n",
    "\n",
    "    moves = {\n",
    "        env.UP:    (1, 0),\n",
    "        env.DOWN:  (-1, 0),\n",
    "        env.RIGHT: (0, 1),\n",
    "        env.LEFT:  (0, -1)\n",
    "    }\n",
    "\n",
    "    for i in range(n):\n",
    "        _, hx, hy = heads[i]\n",
    "        _, fx, fy = fruits[i]\n",
    "\n",
    "        # greedy direction toward fruit\n",
    "        if fx > hx: a = env.UP\n",
    "        elif fx < hx: a = env.DOWN\n",
    "        elif fy > hy: a = env.RIGHT\n",
    "        elif fy < hy: a = env.LEFT\n",
    "        else: a = env.UP\n",
    "\n",
    "        # compute next head position\n",
    "        dx, dy = moves[a]\n",
    "        nx, ny = hx + dx, hy + dy\n",
    "\n",
    "        # check BODY or WALL\n",
    "        if env.boards[i, nx, ny] in (env.BODY, env.WALL):\n",
    "            safe = []\n",
    "            for act, (dx, dy) in moves.items():\n",
    "                tx, ty = hx + dx, hy + dy\n",
    "                if env.boards[i, tx, ty] not in (env.BODY, env.WALL):\n",
    "                    safe.append(act)\n",
    "\n",
    "            if len(safe) == 0:\n",
    "                a = np.random.choice([env.UP, env.DOWN, env.LEFT, env.RIGHT])\n",
    "            else:\n",
    "                a = np.random.choice(safe)\n",
    "\n",
    "        actions[i] = a\n",
    "\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "\n",
    "# Evaluate the baseline policy\n",
    "def evaluate_baseline(steps=1000):\n",
    "    env = get_env(n=NUM_BOARDS)\n",
    "    fruits = []\n",
    "    deaths = []\n",
    "    reward_means = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        actions = baseline_policy(env)\n",
    "        rewards = env.move(actions)\n",
    "        rewards = rewards.numpy().flatten() if hasattr(rewards, \"numpy\") else rewards\n",
    "\n",
    "        # metriche\n",
    "        fruits.append(np.sum(rewards == env.FRUIT_REWARD))\n",
    "        deaths.append(np.sum(rewards == env.HIT_WALL_REWARD))\n",
    "        reward_means.append(np.mean(rewards))\n",
    "\n",
    "    return (\n",
    "        np.mean(np.array(fruits)),\n",
    "        np.mean(np.array(deaths)),\n",
    "        np.mean(np.array(reward_means))\n",
    "    )\n",
    "\n",
    "# Evaluate agent\n",
    "def evaluate_agent(agent, steps=1000):\n",
    "    env = get_env(n=NUM_BOARDS)\n",
    "    fruits = []\n",
    "    deaths = []\n",
    "    rewards_mean = []\n",
    "\n",
    "    for _ in range(steps):\n",
    "        state = env.to_state()  # (N, H, W, C)\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # PPO agent --> has .net\n",
    "            if hasattr(agent, \"net\"):\n",
    "                logits, _ = agent.net(state_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample().cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "            # DDQN agent --> class is DDQNNet\n",
    "            elif isinstance(agent, DDQNNet):\n",
    "                s_tensor_nchw = state_tensor.permute(0, 3, 1, 2)\n",
    "                q_values = agent(s_tensor_nchw)\n",
    "                actions = torch.argmax(q_values, dim=-1).cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "            # A2C agent --> returns (logits, value)\n",
    "            else:\n",
    "                logits, _ = agent(state_tensor)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                actions = dist.sample().cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "        rewards = env.move(actions).cpu().numpy().flatten()\n",
    "\n",
    "        fruits.append(np.sum(rewards == env.FRUIT_REWARD))\n",
    "        deaths.append(np.sum(rewards == env.HIT_WALL_REWARD))\n",
    "        rewards_mean.append(np.mean(rewards))\n",
    "\n",
    "    return (\n",
    "        np.array(fruits),\n",
    "        np.array(deaths),\n",
    "        np.array(rewards_mean)\n",
    "    )\n",
    "\n",
    "# Variables for evaluation and plotting stuff\n",
    "eval_steps = 500                    # Evaluation steps \n",
    "window = 200                        # Size of the window for moving average\n",
    "kernel = np.ones(window) / window   # Convolutional Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050671d",
   "metadata": {},
   "source": [
    "### Load the data (weights and training history)\n",
    "Execute this cell only if training has been already done (for all models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e78d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Weights\n",
    "ppo_agent = PPOAgent(board_size=BOARD_SIZE)\n",
    "ppo_agent.net.load_state_dict(torch.load(\"results/ppo/ppo_weights.pt\"))\n",
    "\n",
    "a2c_agent = A2CNet(board_size=BOARD_SIZE).to(device)\n",
    "a2c_agent.load_state_dict(torch.load(\"results/a2c/a2c_weights.pt\"))\n",
    "\n",
    "ddqn_agent = DDQNNet(board_size=BOARD_SIZE).to(device)\n",
    "ddqn_agent.load_state_dict(torch.load(\"results/ddqn/ddqn_weights.pt\"))\n",
    "\n",
    "# Load training data\n",
    "\n",
    "with open(\"results/ppo/ppo_training.pkl\", \"rb\") as f:\n",
    "    ppo_data = pickle.load(f)\n",
    "\n",
    "ppo_reward_hist = ppo_data[\"reward_hist\"]\n",
    "ppo_fruits_hist = ppo_data[\"fruits_hist\"]\n",
    "ppo_wall_hist = ppo_data[\"deaths_hist\"]\n",
    "\n",
    "with open(\"results/a2c/a2c_training.pkl\", \"rb\") as f:\n",
    "    a2c_data = pickle.load(f)\n",
    "\n",
    "a2c_reward_hist = a2c_data[\"reward_hist\"]\n",
    "a2c_fruits_hist = a2c_data[\"fruits_hist\"]\n",
    "a2c_wall_hist = a2c_data[\"deaths_hist\"]\n",
    "\n",
    "with open(\"results/ddqn/ddqn_training.pkl\", \"rb\") as f:\n",
    "    ddqn_data = pickle.load(f)\n",
    "\n",
    "ddqn_reward_hist = ddqn_data[\"reward_hist\"]\n",
    "ddqn_fruits_hist = ddqn_data[\"fruits_hist\"]\n",
    "ddqn_wall_hist = ddqn_data[\"deaths_hist\"]\n",
    "\n",
    "print(\"All data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa027fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To assess the performance after training, we evaluate the baseline and the agents on eval_steps and compare the average reward, fruits, and deaths. \n",
    "\n",
    "print(\"Evaluation over\", eval_steps, \"steps per agent, using the full trained models.\")\n",
    "\n",
    "# Evaluate Baseline\n",
    "baseline_fruits_avg, baseline_wall_avg, baseline_reward_avg = evaluate_baseline(eval_steps)\n",
    "\n",
    "# Evaluate PPO\n",
    "ppo_fruits_eval, ppo_wall_eval, ppo_reward_eval = evaluate_agent(ppo_agent, eval_steps)\n",
    "\n",
    "# Evaluate A2C\n",
    "a2c_fruits_eval, a2c_wall_eval, a2c_reward_eval = evaluate_agent(a2c_agent, eval_steps)\n",
    "\n",
    "# Evaluate DDQN\n",
    "ddqn_fruits_eval, ddqn_wall_eval, ddqn_reward_eval = evaluate_agent(ddqn_agent, eval_steps)\n",
    "\n",
    "# Build comparison table\n",
    "df = pd.DataFrame({\n",
    "    \"Metric\": [\"Reward (Avg)\", \"Fruits (Avg)\", \"Wall hits (Avg)\"],\n",
    "    \"PPO\": [\n",
    "        np.mean(ppo_reward_eval),\n",
    "        np.mean(ppo_fruits_eval),\n",
    "        np.mean(ppo_wall_eval)\n",
    "    ],\n",
    "    \"A2C\": [\n",
    "        np.mean(a2c_reward_eval),\n",
    "        np.mean(a2c_fruits_eval),\n",
    "        np.mean(a2c_wall_eval)\n",
    "    ],\n",
    "    \"DDQN\": [\n",
    "        np.mean(ddqn_reward_eval),\n",
    "        np.mean(ddqn_fruits_eval),\n",
    "        np.mean(ddqn_wall_eval)\n",
    "    ],\n",
    "    \"Baseline\": [\n",
    "        baseline_reward_avg,\n",
    "        baseline_fruits_avg,\n",
    "        baseline_wall_avg\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ce3ee",
   "metadata": {},
   "source": [
    "### Training Results and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e39dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FIGURE 1 — DDQN Training Curves\n",
    "# ===============================\n",
    "\n",
    "# DDQN moving averages\n",
    "ddqn_reward_moving = np.convolve(ddqn_reward_hist, kernel, mode='valid')\n",
    "ddqn_fruits_moving = np.convolve(ddqn_fruits_hist, kernel, mode='valid')\n",
    "ddqn_wall_moving = np.convolve(ddqn_wall_hist, kernel, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# DDQN – Reward\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ddqn_reward_hist, color='lightblue', alpha=0.5, linewidth=1, label=\"Raw Reward\")\n",
    "plt.plot(ddqn_reward_moving, color='blue', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_reward_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"DDQN – Reward per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# DDQN – Fruits\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ddqn_fruits_hist, color='lightgreen', alpha=0.5, linewidth=1, label=\"Raw Fruits\")\n",
    "plt.plot(ddqn_fruits_moving, color='green', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_fruits_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"DDQN – Fruits per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Fruits\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# DDQN – Wall Collisions\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ddqn_wall_hist, color='lightcoral', alpha=0.5, linewidth=1, label=\"Raw Collisions\")\n",
    "plt.plot(ddqn_wall_moving, color='red', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_wall_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"DDQN – Wall Collisions per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Wall Collisions\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/ddqn_training_curves.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d88bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FIGURE 2 — A2C Training Curves\n",
    "# ==============================\n",
    "\n",
    "# A2C moving averages\n",
    "a2c_reward_moving = np.convolve(a2c_reward_hist, kernel, mode='valid')\n",
    "a2c_fruits_moving = np.convolve(a2c_fruits_hist, kernel, mode='valid')\n",
    "a2c_wall_moving = np.convolve(a2c_wall_hist, kernel, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# A2C – Reward\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(a2c_reward_hist, color='lightblue', alpha=0.5, linewidth=1, label=\"Raw Reward\")\n",
    "plt.plot(a2c_reward_moving, color='blue', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_reward_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"A2C – Reward per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A2C – Fruits\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(a2c_fruits_hist, color='lightgreen', alpha=0.5, linewidth=1, label=\"Raw Fruits\")\n",
    "plt.plot(a2c_fruits_moving, color='green', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_fruits_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"A2C – Fruits per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Fruits\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A2C – Wall Collisions\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(a2c_wall_hist, color='lightcoral', alpha=0.5, linewidth=1, label=\"Raw Collisions\")\n",
    "plt.plot(a2c_wall_moving, color='red', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_wall_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"A2C – Wall Collisions per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Wall Collisions\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/a2c_training_curves.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94fc098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# FIGURE 3 — PPO Training Curves\n",
    "# ==============================\n",
    "\n",
    "# PPO moving averages\n",
    "ppo_reward_moving = np.convolve(ppo_reward_hist, kernel, mode='valid')\n",
    "ppo_fruits_moving = np.convolve(ppo_fruits_hist, kernel, mode='valid')\n",
    "ppo_wall_moving = np.convolve(ppo_wall_hist, kernel, mode='valid')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# PPO – Reward\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(ppo_reward_hist, color='lightblue', alpha=0.5, linewidth=1, label=\"Raw Reward\")\n",
    "plt.plot(ppo_reward_moving, color='blue', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_reward_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"PPO – Reward per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# PPO – Fruits\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(ppo_fruits_hist, color='lightgreen', alpha=0.5, linewidth=1, label=\"Raw Fruits\")\n",
    "plt.plot(ppo_fruits_moving, color='green', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_fruits_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"PPO – Fruits per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Fruits\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# PPO – Wall Collisions\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(ppo_wall_hist, color='lightcoral', alpha=0.5, linewidth=1, label=\"Raw Collisions\")\n",
    "plt.plot(ppo_wall_moving, color='red', linewidth=2.5, label=\"Moving Average\")\n",
    "plt.axhline(y=baseline_wall_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"PPO – Wall Collisions per Step\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Wall Collisions\")\n",
    "plt.legend(fontsize='x-small')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/ppo_training_curves.pdf\", bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b85b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL COMPARISON PLOTS (PPO vs A2C vs DDQN)\n",
    "\n",
    "plt.figure(figsize=(18, 10))\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "# -----------------------------\n",
    "# COMPARISON 1 — Reward\n",
    "# -----------------------------\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(ppo_reward_moving, label=\"PPO\", linewidth=2.5)\n",
    "plt.plot(a2c_reward_moving, label=\"A2C\", linewidth=2.5)\n",
    "plt.plot(ddqn_reward_moving, label=\"DDQN\", linewidth=2.5)\n",
    "plt.axhline(y=baseline_reward_avg, color='black', linestyle='--', linewidth=2,\n",
    "            label=\"Baseline\")\n",
    "plt.title(\"Reward per Step – Comparison\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# -----------------------------\n",
    "# COMPARISON 2 — Fruits\n",
    "# -----------------------------\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(ppo_fruits_moving, label=\"PPO\", linewidth=2.5)\n",
    "plt.plot(a2c_fruits_moving, label=\"A2C\", linewidth=2.5)\n",
    "plt.plot(ddqn_fruits_moving, label=\"DDQN\", linewidth=2.5)\n",
    "plt.axhline(y=baseline_fruits_avg, color='black', linestyle='--', linewidth=2,\n",
    "            label=\"Baseline\")\n",
    "plt.title(\"Fruits per Step – Comparison\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Fruits\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# -----------------------------\n",
    "# COMPARISON 3 — Wall Collisions\n",
    "# -----------------------------\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.plot(ppo_wall_moving, label=\"PPO\", linewidth=2.5)\n",
    "plt.plot(a2c_wall_moving, label=\"A2C\", linewidth=2.5)\n",
    "plt.plot(ddqn_wall_moving, label=\"DDQN\", linewidth=2.5)\n",
    "plt.axhline(y=baseline_wall_avg, color='black', linestyle='--', linewidth=2, label=\"Baseline\")\n",
    "plt.title(\"Wall Collisions per Step – Comparison\")\n",
    "plt.xlabel(\"Training Step\")\n",
    "plt.ylabel(\"Wall Collisions\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/full_training_comparison.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362c8fa",
   "metadata": {},
   "source": [
    "### Play Snake with PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae990465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_game(model, max_steps=100):\n",
    "    game_env = get_env(n=1)\n",
    "    state = game_env.to_state()\n",
    "    frames = []\n",
    "\n",
    "    frames.append(game_env.boards[0].copy())\n",
    "\n",
    "    print(\"Generating animation...\", end=\"\")\n",
    "\n",
    "    # Put model in eval mode\n",
    "    if hasattr(model, \"net\"):      # PPOAgent\n",
    "        model.net.eval()\n",
    "    else:                          # A2CNet or DDQNNet\n",
    "        model.eval()\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # PPO agent --> has .net\n",
    "            if hasattr(model, \"net\"):\n",
    "                logits, _ = model.net(state_tensor)\n",
    "                actions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            # DDQN agent --> class is DDQNNet\n",
    "            elif isinstance(model, DDQNNet):\n",
    "                s_tensor_nchw = state_tensor.permute(0, 3, 1, 2)\n",
    "                q_values = model(s_tensor_nchw)\n",
    "                actions = torch.argmax(q_values, dim=1)\n",
    "\n",
    "            # A2C agent --> returns (logits, value)\n",
    "            else:\n",
    "                logits, _ = model(state_tensor)\n",
    "                actions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        actions_np = actions.cpu().numpy().reshape(-1, 1)\n",
    "\n",
    "        game_env.move(actions_np)\n",
    "        state = game_env.to_state()\n",
    "\n",
    "        frames.append(game_env.boards[0].copy())\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 1. STATIC SNAPSHOTS EVERY 10 STEPS\n",
    "    # -----------------------------------------\n",
    "    snapshot_steps = [0, 10, 20, 30, 40]\n",
    "    snapshot_steps = [s for s in snapshot_steps if s < len(frames)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(snapshot_steps), figsize=(15, 3))\n",
    "\n",
    "    for ax, step in zip(axes, snapshot_steps):\n",
    "        ax.imshow(frames[step], origin='lower', cmap='viridis', vmin=0, vmax=4)\n",
    "        ax.set_title(f\"Step {step}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # -----------------------------------------\n",
    "    # 2. ANIMATION\n",
    "    # -----------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    plt.axis('off')\n",
    "\n",
    "    img = ax.imshow(frames[0], origin='lower', cmap='viridis', vmin=0, vmax=4)\n",
    "    ax.set_title(\"Snake Agent Replay\")\n",
    "\n",
    "    def update(frame):\n",
    "        img.set_array(frame)\n",
    "        return [img]\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=frames,\n",
    "        interval=100,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    # Works only inside a notebook!\n",
    "    return HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_game(a2c_agent, max_steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rl-env)",
   "language": "python",
   "name": "rl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
